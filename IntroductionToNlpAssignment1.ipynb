{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # JSON encoder and decoder: store python data structures (e.g. lists and dictionaries) as text files\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for line in open(r\"C:/Users/shohidul/Desktop/englishtweetssample.json\", \"rt\",encoding=\"utf-8\"):\n",
    "    tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10000\n",
      "10000\n",
      "['Check out my class in #GranblueFantasy! https://t.co/pAvXn8diJr', 'Extending a big Thank You to our Community Partner all over the world! https://t.co/cu7on7g1si', 'Blueberry üç® https://t.co/2gzHAFWYJY', 'RT @LILUZIVERT: Bad day ‚òπÔ∏è¬ÆÔ∏è', \"@prologve_ @BTS_ARMY @BTS_twt I'm Chim tho\", 'i need a dog to cuddle with right now', 'RT: Country Inn countryinns #CampSprings üè® üëâüöñ For Taxi üìû703-445-4450 https://t.co/lXdFUm4qUb']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(tweets))\n",
    "\n",
    "documents = [document[\"text\"] for document in tweets] # right now we only need the text field for each document\n",
    "print(len(documents))\n",
    "print(documents[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my class in # GranblueFantasy !\n",
      "https://t.co/pAvXn8diJr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ufal.udpipe as udpipe\n",
    "\n",
    "model = udpipe.Model.load(r\"C:/Users/shohidul/Desktop/en.segmenter.udpipe\")\n",
    "pipeline = udpipe.Pipeline(model,\"tokenize\",\"none\",\"none\",\"horizontal\") # horizontal: returns one sentence per line, with words separated by a single space\n",
    "segmented_document = pipeline.process(documents[0])\n",
    "\n",
    "print(segmented_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common tokens: [('@', 717), (':', 716), ('RT', 612), ('.', 360), ('the', 281), ('#', 280), (',', 247), ('‚Ä¶', 244), ('a', 241), ('to', 228), ('I', 203), ('and', 192), ('you', 178), ('of', 152), ('in', 150), ('for', 137), ('is', 137), ('-', 123), ('!', 108), ('on', 97)]\n",
      "Vocabulary size: 6078\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_counter = Counter()\n",
    "for doc in documents[:1000]: # IMDB documents\n",
    "    tokenized = pipeline.process(doc)\n",
    "    tokens = tokenized.split() # after segmenter, we can do whitespace splitting\n",
    "    token_counter.update(tokens)\n",
    "\n",
    "print(\"Most common tokens:\", token_counter.most_common(20))\n",
    "print(\"Vocabulary size:\", len(token_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shohidul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Number of tokens: 13\n",
      "Tokens: [('@', 717), ('RT', 612), ('#', 280), ('‚Ä¶', 244), (\"'s\", 79), ('‚Äôs', 53), ('Christmas', 44), ('n‚Äôt', 40), ('people', 40), (\"n't\", 40), ('‚Ä¢', 39), ('one', 38), ('amp', 38)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') # download the stopwords dataset\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# take 150 most common words from the IMDB corpus and filter out stop words and punctuation\n",
    "filtered_tokens = []\n",
    "punctuation_chars = '. , : ( ) ! ? \" = & - ; ... \\\\ '.split() # list of punctuation symbols to ignore\n",
    "for word, count in token_counter.most_common(50):\n",
    "    if word.lower() in stopwords.words(\"english\") or word in punctuation_chars:\n",
    "        continue\n",
    "    filtered_tokens.append((word, count))\n",
    "print(\"Number of tokens:\", len(filtered_tokens))\n",
    "print(\"Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my class in #GranblueFantasy! https://t.co/pAvXn8diJr'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc = list()\n",
    "with open(r\"C:/Users/shohidul/Desktop/englishtweetssample.json\", \"rt\",encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        for l in re.split(r\"\\.\\s|\\?\\s|\\!\\s|\\n\",line):\n",
    "            if l:\n",
    "                Doc.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CountVectorizer(stop_words='english', min_df=3, max_df=0.5, ngram_range=(1,2))\n",
    "CvTransform = CV.fit_transform(Doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<33244x112458 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4888238 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CvTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(CvTransform)\n",
    "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98572</th>\n",
       "      <td>twimg com</td>\n",
       "      <td>0.048676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98571</th>\n",
       "      <td>twimg</td>\n",
       "      <td>0.048676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78785</th>\n",
       "      <td>pbs</td>\n",
       "      <td>0.038264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78786</th>\n",
       "      <td>pbs twimg</td>\n",
       "      <td>0.038257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66634</th>\n",
       "      <td>jpg</td>\n",
       "      <td>0.025646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59931</th>\n",
       "      <td>https pbs</td>\n",
       "      <td>0.023213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84711</th>\n",
       "      <td>resize</td>\n",
       "      <td>0.022151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98022</th>\n",
       "      <td>true</td>\n",
       "      <td>0.021858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98607</th>\n",
       "      <td>twitter</td>\n",
       "      <td>0.021143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98615</th>\n",
       "      <td>twitter com</td>\n",
       "      <td>0.020592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              term    weight\n",
       "98572    twimg com  0.048676\n",
       "98571        twimg  0.048676\n",
       "78785          pbs  0.038264\n",
       "78786    pbs twimg  0.038257\n",
       "66634          jpg  0.025646\n",
       "59931    https pbs  0.023213\n",
       "84711       resize  0.022151\n",
       "98022         true  0.021858\n",
       "98607      twitter  0.021143\n",
       "98615  twitter com  0.020592"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_df.sort_values(by='weight', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
