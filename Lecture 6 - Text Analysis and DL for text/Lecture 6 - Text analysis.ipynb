{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text analysis\n",
    "\n",
    "Anton Akusok\n",
    "\n",
    "<anton.akusok@arcada.fi>\n",
    "\n",
    "Slack: `@Anton Akusok`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcement:\n",
    "\n",
    "This will be my last lecture at Arcada. I am moving to Silo.AI in January 2020.\n",
    "\n",
    "Homeworks `>>` Andrey Shcherbakov  andrey.shcherbakov@arcada.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The problem of meaning\n",
    "\n",
    "A word is a token that represents an item or a phenomenon from the real world.\n",
    "\n",
    "Problem: AI does not work with items/concepts of a real world.  \n",
    "It only works with numbers and tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language is dynamic\n",
    "\n",
    "Scientists can give a detailed explanation of word's meaning. But new words appear faster than scientists can formally describe them. Existing words can change meaning, especially in minds of people from different places.\n",
    "\n",
    "Meaning also depends on a context, e.g. \"proficient\" may be synonym of \"good\" - but not always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/pike.png\" alt=\"Drawing\" style=\"height: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad idea:  Words as independent tokens, or Bag-of-Words\n",
    "\n",
    "**Bag-of-words** is a basic idea of encoding words in one-hot-encoding scheme (vector of zeroes with a single +1 at a unique position). Then a sentence or a text can be described by a vector with elements representing counts of corresponding words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One-hot encoding creates words that are absolutely mathematically independent. Also word vectors are insanely large, like 500,000 elements for English language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"img/onehot.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing words by their context\n",
    "\n",
    "Distributional semantics - one of the most successful ideas of modern statistical NLP!\n",
    "> A word's meaning is given by the words that frequently appear close-by.\n",
    "\n",
    "Intuition: For a sentence with 1 missing word, you can easily give a few likely candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/context.png\" alt=\"Drawing\" style=\"width: 1200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word vectors\n",
    "\n",
    "Word vectors are dense vectors, constructed in a special way that words with similar vectors appear in similar contexts.\n",
    "\n",
    "Also known as: *word embeddings* or *word representations*.\n",
    "\n",
    "Question: How would you represent word vectors in Python?\n",
    "\n",
    "<img src=\"img/wordvector.png\" alt=\"Drawing\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![vis](img/wvvisualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors\n",
    "\n",
    "How do word vectors represent meaning?\n",
    "\n",
    "> They don't.\n",
    "\n",
    "We solve the problem of meaning by kicking it out of word representation.  \n",
    "Models that process word vectors will learn a small portion of meaning, that is relevant to their problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec: Framework for learning word vectors\n",
    "\n",
    "Idea:\n",
    "    * Have a large corpus of text\n",
    "    * Fix vocabulary of all words\n",
    "    * Assign random vectors of given length to all words\n",
    "    * Go through each word in all the text, selecting word W and context C\n",
    "    * Compute probability P(W|C)\n",
    "    * Maximize probability P with stochastic gradient descend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting consequences: Word math\n",
    "\n",
    "![wordmath](img/wordmath.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Blackbox learning approach (our course)\n",
    "\n",
    "There is a big difference between knowing how to do something, and being able to do something. We aim at the latter.\n",
    "\n",
    "Let's think of text analysis as a *black-box* environment that just works, even if we don't understand how. And let's learn how to do some useful things today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-study materials\n",
    "\n",
    "If you want to know more, look at Stanford lectures on NLP:  \n",
    "http://web.stanford.edu/class/cs224n/ \n",
    "\n",
    "They have slides, web recordings of actual lectures, and **the math**. We skip the math.  \n",
    "(https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Bag-of-Words\n",
    "\n",
    "This approach can still be useful, if you suspect that your problem can be solved by finding specific related words (and NOT by extracting actual meaning from word order).\n",
    "\n",
    "**The good:** It often works. Scikit-Learn has everything you need. Can work with any language (even HTML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenizing: Splitting text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38082"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word frequencies\n",
    "\n",
    "`CountVectorizer()` returns word *occurrencies*, so frequent words like \"the\" receive very high numbers.\n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency) weights words in a special way. Frequent words across whole dataset are devalued, and frequent words that present only in a small portion of documents are greatly emphasized. \n",
    "\n",
    "This highlights \"specialized\" words that likely carry area-specific meaning, and will be useful in classification or whatever you wanna do with the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_idf = TfidfTransformer().fit(X_counts)\n",
    "X_weights = tf_idf.transform(X_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"the car\"\n",
    "counts = count_vect.transform([text, ])\n",
    "counts.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25933548, 0.9657873 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = tf_idf.transform(counts)\n",
    "weights.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "Bag-of-words representations are very long.\n",
    "\n",
    "`SGDClassifier` is iteratively optimized linear model that works well (although never gets *the perfect* solution like normal linear model).  \n",
    "It converges extremely fast on text vectors.\n",
    "\n",
    "`NaiveBayes` is a good method with only drawback that it assumes independence of input features. But we *already assume independence of words* by using Bag-or-words method, so it does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xt, Xv, Yt, Yv = train_test_split(X_weights, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.807705903145988"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(Xt, Yt)\n",
    "clf.score(Xv, Yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9208200777659951"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier().fit(Xt, Yt)\n",
    "sgd.score(Xv, Yv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Don't forget pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "model = Pipeline([\n",
    "    (\"vect\", CountVectorizer()),\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84490659, 0.84210526, 0.83079646, 0.84083658])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, data.data, data.target, cv=4, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with ANY text (literally)!\n",
    "\n",
    "`HashingVectorizer` counts EVERY sequence of *n* characters as a word.  \n",
    "\n",
    "Of course this creates vectors with billions of elements, so `HashingVectorizer` actually *hashes* its \"words\" into a random place inside a vector of a given lenght. Same \"words\" *always* go to the same place.\n",
    "\n",
    "It can detect names, emails, character combinations, passwords, etc. with enough training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Scikit-Learn` has an advanced `HashingVectorizer()` implementation that can generates *n*-sequencies only within word boundaries or even hash every single word separately, and normalize resulting vectors with `alternate_sign=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HashingVectorizer(alternate_sign=True, analyzer='word', binary=False,\n",
       "                  decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "                  encoding='utf-8', input='content', lowercase=True,\n",
       "                  n_features=1048576, ngram_range=(1, 1), norm='l2',\n",
       "                  preprocessor=None, stop_words=None, strip_accents=None,\n",
       "                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HashingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "hashing = Pipeline([\n",
    "    (\"vect\", HashingVectorizer(n_features=15_000, analyzer=\"char\", \n",
    "                               ngram_range=(4, 5), alternate_sign=False)),\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    (\"clf\", SGDClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88086006, 0.8975627 , 0.87362832, 0.87628501])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(hashing, data.data, data.target, cv=4, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Word vectors: Spacy\n",
    "\n",
    "https://spacy.io is a nice Python library for NLP tasks. \n",
    "\n",
    "It supports a range of language models: https://spacy.io/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Two bananas in pyjamas."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Two bananas in pyjamas.\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Token\n",
    "\n",
    "Token is a meaningful element of text - a word or punctuation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two [ 1.9376e-01 -3.4272e-01 -3.7280e-01 -1.5344e-01  2.6030e-01 -2.5268e-01\n",
      " -2.3870e-01 -7.9489e-02  3.9787e-01  2.5414e+00  3.0602e-01  4.0473e-02\n",
      "  2.2262e-01 -2.9280e-01 -7.7424e-02 -1.2155e-01 -1.5803e-02  1.1511e+00\n",
      " -6.2032e-02 -1.1371e-01 -4.5909e-01  6.8800e-02  4.8372e-04  6.5954e-02\n",
      "  1.2627e-01 -2.4380e-01 -3.7474e-01 -1.3026e-01  3.3211e-01  1.7395e-01\n",
      " -1.6609e-02  5.1471e-01  2.5513e-01 -6.3719e-03  2.4802e-01 -9.9860e-02\n",
      "  4.1756e-02 -3.2667e-01 -4.7819e-02 -3.1468e-01 -7.5799e-02  1.6569e-01\n",
      " -1.2541e-01 -8.8889e-02 -2.8187e-02 -1.6542e-02 -6.5677e-02  2.1452e-01\n",
      "  3.0963e-02  4.4404e-03 -2.3115e-01  3.6552e-01 -1.1054e-01  2.7337e-02\n",
      " -4.1860e-01  8.6131e-02 -8.8410e-02  3.7810e-01  9.4435e-02  3.2761e-01\n",
      "  5.6099e-01  3.7211e-02  1.6627e-01  5.5609e-01 -1.5403e-01  1.3870e-01\n",
      "  2.1802e-01  2.8899e-01  3.1493e-02  4.6057e-01  4.6414e-01 -1.8594e-01\n",
      "  2.8329e-01  2.5512e-01 -3.4265e-02 -5.8959e-02 -3.2013e-02 -1.7995e-01\n",
      "  1.6876e-01 -1.1089e-01 -1.5121e-01  8.4075e-02 -3.0209e-01 -1.6094e-01\n",
      " -2.0831e-01 -1.8030e-02 -5.1268e-01 -8.1036e-03  6.8853e-02  1.1491e-02\n",
      " -1.3611e-01  9.5693e-02  9.1562e-03 -3.7841e-01  2.6085e-02  1.8553e-01\n",
      "  3.3894e-01 -3.8998e-03 -5.0647e-02 -2.3945e-01 -1.8099e-01  3.4137e-01\n",
      "  9.9227e-02 -3.2175e-02 -1.1529e-01 -1.0975e+00  4.2491e-02  1.4214e-01\n",
      "  2.1709e-01  1.1780e-01  1.9512e-01 -3.9119e-01  1.8497e-01  3.4724e-01\n",
      "  1.6102e-01  2.3620e-02 -2.7990e-01  8.5623e-01 -5.0472e-02  4.6425e-01\n",
      "  1.4560e-01 -4.5258e-01  2.6008e-01 -7.3755e-02 -2.4108e-02  2.1419e-02\n",
      " -2.7702e-02 -7.8651e-02 -5.2932e-02  2.6219e-01 -2.1096e-01  6.1365e-01\n",
      "  3.6502e-01 -2.7366e-02  2.4766e-01  2.5812e-01  4.3982e-01  1.7060e-01\n",
      "  2.8616e-01  2.2053e-02 -1.3821e+00 -1.8840e-01  3.6154e-02  7.2073e-02\n",
      "  1.2499e-01  1.2907e-01 -6.1760e-01 -1.5952e-01  4.3559e-02 -2.3181e-01\n",
      " -1.5959e-01  5.0421e-02 -1.8784e-03  2.8262e-01 -1.5699e-01 -2.5379e-01\n",
      " -3.0523e-01  4.1323e-01 -2.3686e-01 -2.1239e-01 -2.3961e-01  7.4232e-02\n",
      " -5.3271e-02 -3.6116e-01 -2.6260e-01 -2.2047e-01 -1.1457e-01 -1.0431e-01\n",
      "  1.6839e-01  6.1315e-02 -1.6197e-01  1.4748e-01  6.5418e-02  3.2589e-01\n",
      " -4.4315e-01 -2.5054e-01 -1.4625e-01 -2.8516e-02  2.0639e-01 -2.2743e-01\n",
      "  1.5513e-02  1.3105e-01 -1.7792e-01 -1.2199e-01 -3.4171e-01  1.3275e-01\n",
      " -8.3057e-03 -2.1780e-01 -2.0744e-01  2.2949e-01  7.5315e-04  1.7829e-01\n",
      " -7.4869e-02  9.4627e-02 -1.8343e-01  8.0051e-03 -3.8751e-01 -6.9959e-03\n",
      " -4.1404e-01 -1.6475e-01 -3.4728e-01 -2.3360e-01  1.1477e-01  1.4920e-01\n",
      "  4.0840e-01 -1.7635e-01 -1.4627e-01  8.0708e-03  8.4708e-03  2.7669e-01\n",
      " -1.1116e-01  3.0369e-01 -1.8142e-01 -4.9423e-01 -4.1393e-01  8.9950e-03\n",
      " -3.1975e-01 -1.3464e-01  1.1468e-01  2.9114e-02 -2.5939e-01 -5.8733e-03\n",
      "  4.1721e-02  1.7765e-01  1.2825e-01 -7.4444e-02  6.2456e-02  1.4735e-01\n",
      " -5.0376e-02  2.8929e-01 -1.1307e-01 -5.8742e-02  1.0932e-01  4.0083e-01\n",
      "  1.2619e-01 -4.3845e-01  2.0018e-01  2.2035e-01  7.4264e-02  4.9342e-01\n",
      "  1.3417e-02  8.1612e-02 -2.2268e-01  4.9461e-01  1.2823e-01 -1.9331e-01\n",
      " -2.6884e-01 -2.1927e-02 -1.8295e-01  2.6601e-01 -1.4650e-01 -7.3029e-02\n",
      " -1.7796e-02 -1.4582e-01  8.5168e-02  3.4017e-01  1.0176e-01  2.1184e-01\n",
      " -2.0574e-01  1.2020e-02  6.2379e-01 -7.0523e-02 -6.2970e-01  9.6654e-02\n",
      " -1.6987e-01  1.5421e-01  3.0537e-01  2.4742e-01 -2.2914e-02 -5.7210e-01\n",
      " -3.9433e-02 -2.6464e-01 -1.6404e-01 -3.7273e-01  3.2102e-03  7.7683e-02\n",
      "  2.3106e-01  1.8220e-01  2.8840e-01  1.8396e-01  2.2587e-01  5.1461e-01\n",
      " -6.6029e-01 -9.2077e-02  6.5315e-02  2.0683e-01  2.0572e-01 -1.5546e-01\n",
      " -5.4668e-03  3.2497e-01 -3.1691e-01 -9.7099e-02  1.2052e-01  2.4502e-02\n",
      " -3.5260e-01  1.0662e-02  1.0179e-01 -5.1226e-01  2.8686e-01 -3.8719e-01]\n",
      "bananas [-2.2009e-01 -3.0322e-02 -7.9859e-02 -4.6279e-01 -3.8600e-01  3.6962e-01\n",
      " -7.7178e-01 -1.1529e-01  3.3601e-02  5.6573e-01 -2.4001e-01  4.1833e-01\n",
      "  1.5049e-01  3.5621e-01 -2.1508e-01 -4.2743e-01  8.1400e-02  3.3916e-01\n",
      "  2.1637e-01  1.4792e-01  4.5811e-01  2.0966e-01 -3.5706e-01  2.3800e-01\n",
      "  2.7971e-02 -8.4538e-01  4.1917e-01 -3.9181e-01  4.0434e-04 -1.0662e+00\n",
      "  1.4591e-01  1.4643e-03  5.1277e-01  2.6072e-01  8.3785e-02  3.0340e-01\n",
      "  1.8579e-01  5.9999e-02 -4.0270e-01  5.0888e-01 -1.1358e-01 -2.8854e-01\n",
      " -2.7068e-01  1.1017e-02 -2.2217e-01  6.9076e-01  3.6459e-02  3.0394e-01\n",
      "  5.6989e-02  2.2733e-01 -9.9473e-02  1.5165e-01  1.3540e-01 -2.4965e-01\n",
      "  9.8078e-01 -8.0492e-01  1.9326e-01  3.1128e-01  5.5390e-02 -4.2423e-01\n",
      " -1.4082e-02  1.2708e-01  1.8868e-01  5.9777e-02 -2.2215e-01 -8.3950e-01\n",
      "  9.1987e-02  1.0180e-01 -3.1299e-01  5.5083e-01 -3.0717e-01  4.4201e-01\n",
      "  1.2666e-01  3.7643e-01  3.2333e-01  9.5673e-02  2.5083e-01 -6.4049e-02\n",
      "  4.2143e-01 -1.9375e-01  3.8026e-01  7.0883e-03 -2.0371e-01  1.5402e-01\n",
      " -3.7877e-03 -2.9396e-01  9.6518e-01  2.0068e-01 -5.6572e-01 -2.2581e-01\n",
      "  3.2251e-01 -3.4634e-01  2.7064e-01 -2.0687e-01 -4.7229e-01  3.1704e-01\n",
      " -3.4665e-01 -2.5188e-01 -1.1201e-01 -3.3937e-01  3.1518e-01 -3.2221e-01\n",
      " -2.4530e-01 -7.1571e-02 -4.3971e-01 -1.2070e+00  3.3365e-01 -5.8208e-02\n",
      "  8.0899e-01  4.2335e-01  3.8678e-01 -6.0797e-01 -7.3760e-01 -2.0547e-01\n",
      " -1.7499e-01 -3.7842e-03  2.1930e-01 -5.2486e-02  3.4869e-01  4.3852e-01\n",
      " -3.4471e-01  2.8910e-01  7.2554e-02 -4.8625e-01 -3.8390e-01 -4.4760e-01\n",
      "  4.3278e-01 -2.7128e-03 -9.0067e-01 -3.0819e-02 -3.8630e-01 -8.0798e-02\n",
      " -1.6243e-01  2.8830e-01 -2.6349e-01  1.7628e-01  3.5958e-01  5.7672e-01\n",
      " -5.4624e-01  3.8555e-02 -2.0182e+00  3.2916e-01  3.4672e-01  1.5398e-01\n",
      " -4.3446e-01 -4.1428e-02 -6.9588e-02  5.1513e-01 -1.3489e-01 -5.7239e-02\n",
      "  4.9241e-01  1.8643e-01  3.8596e-01 -3.7329e-02 -5.4216e-01 -1.8152e-01\n",
      "  4.3110e-01 -4.6967e-01  6.6801e-02  5.0323e-01 -2.4059e-01  3.6742e-01\n",
      "  2.9300e-01 -8.7883e-02 -4.7940e-01 -4.3431e-02 -2.6137e-01 -6.2658e-01\n",
      "  1.1446e-01  2.7682e-01  3.4800e-01  5.0018e-01  1.4269e-01 -3.3545e-01\n",
      " -3.9712e-01 -3.3121e-01 -3.4434e-01 -4.1627e-01 -3.5707e-03 -6.2350e-01\n",
      "  3.7794e-01 -1.6765e-01 -4.1954e-01 -3.3134e-01  3.1232e-01 -3.9494e-01\n",
      " -4.6921e-03 -4.8884e-01 -2.2059e-02 -2.6174e-01  1.7937e-01  3.6628e-01\n",
      "  5.8971e-02 -3.5991e-01 -4.4393e-01 -1.1890e-01  3.3487e-01  3.6505e-02\n",
      " -3.2788e-01  3.3425e-01 -5.6361e-01 -1.1190e-01  5.3770e-01  2.0311e-01\n",
      "  1.5110e-01  1.0623e-02  3.3401e-01  4.6084e-01  5.6293e-01 -7.5432e-02\n",
      "  5.4813e-01  1.9395e-01 -2.6265e-01 -3.1699e-01 -8.1778e-01  5.8169e-02\n",
      " -5.7866e-02 -1.1781e-01 -5.8742e-02 -1.4092e-01 -9.9394e-01 -9.4532e-02\n",
      "  2.3503e-01 -4.9027e-01  8.5832e-01  1.1540e-01 -1.5049e-01  1.9065e-01\n",
      " -2.6705e-01  2.5326e-01 -6.7579e-01 -1.0633e-02 -5.5158e-02 -3.1004e-01\n",
      " -5.8036e-02 -1.7200e-01  1.3298e-01 -3.2899e-01 -7.5481e-02  2.9425e-02\n",
      " -3.2949e-01 -1.8691e-01 -9.5323e-01 -3.5468e-01 -3.3162e-01  5.6441e-02\n",
      "  2.1790e-02  1.7182e-01 -4.4267e-01  6.9765e-01 -2.6876e-01  1.1659e-01\n",
      " -1.6584e-01  3.8296e-01  2.9109e-01  3.6318e-01  3.6961e-01  1.6305e-01\n",
      "  1.8152e-01  2.2453e-01  3.9866e-02 -3.7607e-02 -3.6089e-01  7.0818e-02\n",
      " -2.1509e-01  3.6551e-01 -5.1603e-01 -5.8102e-03 -4.8320e-01 -2.5068e-01\n",
      " -5.2062e-02 -2.0828e-01  2.9060e-01  2.2084e-02 -6.8123e-01  4.2063e-01\n",
      "  9.5973e-02  8.1720e-01 -1.5241e-01  6.2994e-01  2.6449e-01 -1.3516e-01\n",
      "  3.2450e-01  3.0503e-01  1.2357e-01  1.5107e-01  2.8327e-01 -3.3838e-01\n",
      "  4.6106e-02 -1.2361e-01  1.4516e-01 -2.7947e-02  2.6231e-02 -5.9591e-01\n",
      " -4.4183e-01  7.8440e-01 -3.4375e-02 -1.3928e+00  3.5248e-01  6.5220e-01]\n",
      "in [ 8.9187e-02  2.5792e-01  2.6282e-01 -2.9365e-02  4.7187e-01 -1.0389e-01\n",
      " -1.0013e-01  8.1230e-02  2.0883e-01  2.5726e+00 -6.7854e-01  3.6121e-02\n",
      "  1.3085e-01  1.2462e-03  1.4769e-01  2.6926e-01  3.7144e-01  1.3501e+00\n",
      " -1.1326e-01 -2.3036e-01 -2.6575e-01 -1.8077e-01  9.2455e-02 -1.6215e-01\n",
      "  1.5003e-01 -3.4547e-01  7.2295e-02  4.0659e-01  1.0021e-02 -7.9257e-03\n",
      " -1.1435e-01  1.7008e-02 -2.9789e-01  1.9079e-01  3.7112e-01 -2.6588e-01\n",
      "  1.6212e-01  6.5469e-02 -3.1781e-01 -3.2260e-02  8.1969e-02  3.4450e-01\n",
      " -1.7362e-01 -3.5745e-01  5.4487e-02  3.9941e-01  1.3699e-01 -2.2066e-02\n",
      "  1.1025e-01 -4.1898e-01  1.2760e-01 -9.5869e-02 -1.7944e-01 -1.7443e-01\n",
      "  2.7302e-01 -1.9464e-01  2.6747e-01 -2.8241e-01  1.6380e-01 -1.1518e-01\n",
      "  1.3196e-02 -1.0616e-01 -3.6093e-01  2.3634e-02  1.3464e-01  2.1652e-02\n",
      " -2.7094e-01 -1.8737e-02  1.0017e-01  3.6071e-01 -9.3951e-02  4.7634e-01\n",
      "  1.2874e-01  1.1868e-03  1.3770e-01 -1.4034e-01 -1.8870e-01 -1.6405e-01\n",
      " -1.5349e-01  3.2347e-01 -1.7616e-01  3.5230e-01 -2.3531e-02 -1.9121e-01\n",
      " -5.4809e-02 -9.9521e-02 -3.0056e-01  3.6632e-01 -2.1509e-01  7.4123e-02\n",
      " -2.0267e-01  1.2860e-01 -3.8111e-01 -2.5482e-02  4.5103e-01  8.8633e-02\n",
      "  3.6288e-01 -2.3406e-01 -8.6024e-02 -5.0604e-01  3.4242e-02  4.3998e-01\n",
      " -8.3023e-02 -1.1969e-01  6.8686e-01 -3.4115e-01  2.1228e-01  4.0039e-01\n",
      "  2.6367e-01 -3.7144e-01  1.6206e-01 -4.2854e-01  7.8658e-02 -2.9050e-01\n",
      "  2.1727e-01 -2.7484e-01  3.5887e-01  2.7055e-01 -1.1326e-01 -1.4848e-01\n",
      " -5.0659e-03 -7.6862e-02  7.8621e-02 -2.4922e-01  4.2026e-01 -6.9698e-02\n",
      "  7.1595e-02  7.1665e-03  2.7473e-01 -1.5664e-01  2.5713e-01 -5.8461e-02\n",
      " -2.9733e-01 -9.0996e-02  5.2460e-01  1.4889e-01 -2.0883e-01 -1.3004e-01\n",
      " -2.0022e-01  4.5030e-01 -3.4654e-01 -2.6007e-01  3.5247e-01 -3.4757e-01\n",
      "  3.3738e-02  1.9907e-01 -3.2912e-01 -8.4689e-02  6.5319e-01  2.0954e-01\n",
      "  7.9274e-02  1.0860e-01  2.6466e-03 -1.2843e-01 -2.2811e-01  5.1501e-02\n",
      " -2.7429e-01  1.4505e-01 -1.8430e-01 -3.4825e-01 -1.1701e-01  3.4034e-01\n",
      "  7.5848e-02  8.2390e-02 -3.9188e-01 -2.2312e-02 -8.0373e-02  1.4477e-01\n",
      "  2.9701e-01 -1.0523e-01  9.2893e-02  2.9813e-02 -1.1761e-01  1.6308e-01\n",
      "  9.8382e-02  4.6152e-01 -1.6200e-01 -2.4560e-01  2.0293e-01 -1.1344e-01\n",
      "  5.7902e-02 -1.9528e-01 -2.0141e-01 -2.2874e-01 -1.4101e-02  2.6370e-01\n",
      " -1.0028e-01 -5.1896e-02  1.8859e-01 -1.7767e-01 -1.1556e-01  1.2100e-01\n",
      "  1.7303e-01  1.1773e-01  3.4837e-02  2.8485e-01 -3.0447e-01  6.1024e-02\n",
      " -2.6442e-01 -8.1135e-02 -4.4524e-02 -3.6931e-02 -1.5217e-01  2.9175e-01\n",
      "  4.4926e-01 -2.8875e-01  3.3193e-01 -1.2420e-02 -1.8805e-01 -1.9832e-01\n",
      " -1.9736e-01  2.6893e-01  1.1106e-01 -6.7383e-01 -1.5180e-01 -1.6615e-01\n",
      " -1.6563e-01  9.3671e-03 -1.5945e-01 -3.3468e-01  2.2038e-01 -1.6724e-01\n",
      " -1.5350e-01 -6.1782e-01 -1.7258e-01  8.8928e-02  1.9411e-02  1.8296e-01\n",
      "  3.2967e-01 -2.4906e-03 -9.2080e-02  5.1400e-01  4.2484e-03 -8.4377e-02\n",
      " -7.1448e-01 -2.2148e-01 -4.8350e-02  4.3761e-02 -2.9376e-01 -2.2287e-01\n",
      "  1.8001e-01  7.2197e-02  4.6499e-01  5.6466e-02  4.0844e-01 -2.3641e-01\n",
      " -3.8946e-02  8.7363e-02 -2.1901e-01 -3.2310e-01 -1.9989e-01 -3.1280e-01\n",
      " -6.7656e-02 -2.2596e-01  9.0926e-02  2.8365e-01  3.1462e-01  4.6082e-01\n",
      " -2.4871e-02 -1.4605e-01  3.0454e-01  1.7704e-01 -1.1311e-02  2.6807e-01\n",
      " -3.2461e-02 -1.6644e-01 -1.5313e-01 -2.0426e-01 -3.0820e-01 -2.4590e-01\n",
      "  8.5848e-02 -1.1767e-01 -6.3056e-02 -1.8133e-01 -1.8629e-01 -1.7694e-01\n",
      "  2.9618e-01  3.5987e-01  2.0102e-03  3.8616e-01  3.6712e-01 -5.5112e-02\n",
      " -3.4733e-01 -7.2678e-02 -5.1119e-02 -2.9069e-01  5.3598e-02  1.9587e-02\n",
      "  1.6808e-01 -2.7456e-01 -9.7179e-02 -5.4541e-02  1.9229e-01 -4.8128e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -2.0304e-01  1.9368e-01 -3.2546e-01  1.4421e-01 -1.6900e-01  2.6501e-01]\n",
      "pyjamas [ 1.9095e-01 -4.9804e-01 -2.6771e-01 -5.6022e-02 -1.7520e-01  1.9237e-01\n",
      "  1.1580e-01 -4.3959e-01 -4.0391e-01  4.8932e-01 -2.1835e-01  8.2319e-02\n",
      " -1.2638e-01 -1.8102e-02  1.0808e-01 -8.7456e-02  1.5171e-02  3.0357e-01\n",
      " -1.0834e-01 -5.5606e-01 -6.7118e-01 -5.6832e-01 -3.6537e-01  1.1583e-01\n",
      "  5.5654e-02 -2.3239e-01 -1.3381e-01  2.6839e-02  3.1981e-02  3.3165e-01\n",
      "  8.3014e-02  2.9282e-01 -1.0282e-01  5.6327e-01  4.2352e-02 -9.5268e-01\n",
      "  4.1784e-01  1.0255e-01  1.0748e-03  2.4993e-01  4.2311e-01 -2.3822e-01\n",
      " -5.5894e-01  1.3366e-01 -1.3233e-01 -1.6066e-01  2.5015e-01  5.3932e-01\n",
      "  8.3208e-01  3.8616e-01  5.0471e-01  2.2545e-02 -1.6626e-01 -3.5128e-01\n",
      "  7.2978e-01 -1.0944e-01  3.8559e-01 -1.7194e-01  3.6662e-01 -1.5815e-01\n",
      " -4.7744e-01 -7.2209e-01  4.7908e-01  1.4687e-01 -8.8191e-02 -2.4622e-01\n",
      " -7.2795e-01 -1.4420e-01  8.2687e-02  3.3889e-01 -8.0232e-03  4.8050e-01\n",
      "  5.9092e-01 -9.5002e-02 -1.1985e-02 -1.7913e-01  8.7388e-01  2.2641e-01\n",
      " -3.4686e-01 -2.4945e-01 -1.3986e-01 -1.5316e-02  2.4742e-01  1.5188e-02\n",
      "  3.9805e-01 -3.2945e-02  1.0513e+00  7.5412e-01  6.3684e-01  9.9714e-01\n",
      " -1.9657e-02  5.3149e-01  7.6202e-01  6.1693e-02 -2.8118e-01 -1.3253e-01\n",
      "  2.7659e-01 -3.1184e-01 -1.3438e-02 -3.7067e-01 -8.4238e-02  6.7020e-01\n",
      " -6.2565e-02  2.3434e-01 -7.5466e-01 -8.3883e-01 -5.1438e-01  2.5986e-01\n",
      " -2.0209e-01  1.6822e-01  2.9302e-01 -8.2876e-01 -2.6414e-01  7.9456e-01\n",
      " -3.1582e-01  1.8008e-01  4.8746e-01 -3.0213e-01 -6.3549e-02 -3.8507e-01\n",
      " -2.2270e-01  1.1543e-01 -1.6685e-01  2.4426e-02 -5.2307e-02  8.9989e-04\n",
      " -1.4478e-01  2.5788e-01  1.7110e-01 -1.8499e-01  4.6454e-01 -7.7766e-02\n",
      " -5.3696e-01  2.2980e-01  3.2494e-01  4.5648e-01  1.0190e-01 -5.4127e-01\n",
      " -3.7443e-01  6.9407e-01 -1.8062e+00  3.0752e-01 -6.6789e-02 -4.1782e-01\n",
      " -2.1140e-02  2.7230e-01  5.7343e-01 -2.4131e-02  5.6763e-02  5.6678e-01\n",
      " -4.4556e-02 -1.4333e-01 -7.1552e-01  8.6379e-02 -2.7198e-01 -1.3002e-01\n",
      " -5.5879e-01 -4.8606e-01 -2.6078e-01  5.5276e-02 -5.0061e-01 -2.7547e-01\n",
      " -7.9320e-02  6.1690e-01  6.4115e-02 -2.5754e-01 -4.6606e-01  9.7822e-03\n",
      " -4.4321e-01 -2.4469e-01 -4.3855e-01  2.9539e-01  5.9413e-02 -5.6969e-01\n",
      " -4.2959e-01  4.0847e-02  2.9597e-01  3.5430e-01  2.8442e-01 -6.1801e-02\n",
      " -4.3874e-01 -2.1623e-01 -2.8974e-01  7.4236e-02  5.7956e-03  3.0944e-01\n",
      " -3.2091e-01  3.3476e-01  2.9968e-01  3.7682e-01  1.5826e-01  2.6105e-01\n",
      "  2.8336e-01 -1.7855e-01 -1.7157e-01  1.7598e-02  3.5367e-01 -3.5682e-01\n",
      " -1.2706e-01  9.2040e-02 -1.8874e-01  4.3522e-01 -9.7409e-02 -5.3781e-02\n",
      "  2.2562e-02  8.4106e-01 -1.4843e-01 -7.0785e-01 -1.1591e+00  5.0245e-01\n",
      " -9.6274e-02  1.1321e-01 -5.3123e-01 -9.0501e-01  9.3204e-01  5.7394e-01\n",
      " -3.7887e-01 -1.1189e-01 -8.3345e-01 -3.1955e-02 -5.9868e-01  2.9226e-01\n",
      "  1.5138e-01 -5.9581e-01 -7.7198e-01 -2.7560e-01 -4.9839e-01  5.1584e-02\n",
      " -1.5968e-01  5.4567e-01  6.0540e-03 -2.0368e-01 -2.7768e-01  2.7800e-01\n",
      "  3.9243e-01 -3.3505e-01 -3.3639e-01 -4.2814e-02  6.0105e-01  4.7123e-01\n",
      " -8.3515e-01 -3.2935e-01 -6.0728e-01  7.6027e-02  1.5844e-01  3.5912e-01\n",
      "  8.9413e-02 -4.4236e-01  1.5149e-01  5.7970e-01 -9.1408e-02 -2.1812e-01\n",
      " -2.5293e-01  1.5337e-01 -5.8354e-01  2.9364e-03  3.6552e-01  9.9693e-01\n",
      "  1.1455e-01  4.9908e-01 -1.6893e-01  3.8765e-01 -1.7463e-01  3.9725e-01\n",
      "  1.9753e-02  8.3307e-01  1.2761e-01  6.5701e-01 -9.9747e-01 -5.4223e-01\n",
      " -4.4622e-01  3.4348e-01 -1.4075e-01  2.7037e-01  1.4218e-01  1.4678e-01\n",
      "  2.4701e-01 -8.2479e-02  3.9337e-01  1.7096e-01  4.8574e-01  8.1305e-02\n",
      " -4.1013e-01  6.5782e-01 -3.8292e-01 -2.7028e-01  2.0781e-01  1.8368e-02\n",
      " -2.3625e-01 -6.5980e-01  3.2447e-01  3.1187e-01  9.8794e-02 -1.0661e-01\n",
      " -5.8134e-01  2.1290e-01  9.2152e-01  4.1517e-01  3.0281e-01 -2.9454e-01]\n",
      ". [ 0.012001   0.20751   -0.12578   -0.59325    0.12525    0.15975\n",
      "  0.13748   -0.33157   -0.13694    1.7893    -0.47094    0.70434\n",
      "  0.26673   -0.089961  -0.18168    0.067226   0.053347   1.5595\n",
      " -0.2541     0.038413  -0.01409    0.056774   0.023434   0.024042\n",
      "  0.31703    0.19025   -0.37505    0.035603   0.1181     0.012032\n",
      " -0.037566  -0.5046    -0.049261   0.092351   0.11031   -0.073062\n",
      "  0.33994    0.28239    0.13413    0.070128  -0.022099  -0.28103\n",
      "  0.49607   -0.48693   -0.090964  -0.1538    -0.38011   -0.014228\n",
      " -0.19392   -0.11068   -0.014088  -0.17906    0.24509   -0.16878\n",
      " -0.15351   -0.13808    0.02151    0.13699    0.0068061 -0.14915\n",
      " -0.38169    0.12727    0.44007    0.32678   -0.46117    0.068687\n",
      "  0.34747    0.18827   -0.31837    0.4447    -0.2095    -0.26987\n",
      "  0.48945    0.15388    0.05295   -0.049831   0.11207    0.14881\n",
      " -0.37003    0.30777   -0.33865    0.045149  -0.18987    0.26634\n",
      " -0.26401   -0.47556    0.68381   -0.30653    0.24606    0.31611\n",
      " -0.071098   0.030417   0.088119   0.045025   0.20125   -0.21618\n",
      " -0.36371   -0.25948   -0.42398   -0.14305   -0.10208    0.21498\n",
      " -0.21924   -0.17935    0.21546    0.13801    0.24504   -0.2559\n",
      "  0.054815   0.21307    0.2564    -0.25673    0.17961   -0.47638\n",
      " -0.25181   -0.0091498 -0.054362  -0.21007    0.12597   -0.40795\n",
      " -0.021164   0.20585    0.18925   -0.0051896 -0.51394    0.28862\n",
      " -0.077748  -0.27676    0.46567   -0.14225   -0.17879   -0.4357\n",
      " -0.32481    0.15034   -0.058367   0.49652    0.20472    0.019866\n",
      "  0.13326    0.12823   -1.0177     0.29007    0.28995    0.029994\n",
      " -0.10763    0.28665   -0.24387    0.22905   -0.26249   -0.069269\n",
      " -0.17889    0.21936    0.15146    0.04567   -0.050497   0.071482\n",
      " -0.1027    -0.080705   0.30296    0.031302   0.26613   -0.0060951\n",
      "  0.10313   -0.39987   -0.043945  -0.057625   0.08702   -0.098152\n",
      "  0.22835   -0.005211   0.038075   0.01591   -0.20622    0.021853\n",
      "  0.0040426 -0.043063  -0.002294  -0.26097   -0.25802   -0.28158\n",
      " -0.23118   -0.010404  -0.30102   -0.4042     0.014653  -0.10445\n",
      "  0.30377   -0.20957    0.3119     0.068272   0.1008     0.010423\n",
      "  0.54011    0.29865    0.12653    0.013761   0.21738   -0.39521\n",
      "  0.066633   0.50327    0.14913   -0.11554    0.010042   0.095698\n",
      "  0.16607   -0.18808    0.055019   0.026715  -0.3164    -0.046583\n",
      " -0.051591   0.023475  -0.11007    0.085642   0.28394    0.040497\n",
      "  0.071986   0.14157   -0.021199   0.44718    0.20088   -0.12964\n",
      " -0.067183   0.47614    0.13394   -0.17287   -0.37324   -0.17285\n",
      "  0.02683   -0.1316     0.09116   -0.46487    0.1274    -0.090159\n",
      " -0.10552    0.068006  -0.13381    0.17056    0.089509  -0.23133\n",
      " -0.27572    0.061534  -0.051646   0.28377    0.25286   -0.24139\n",
      " -0.19905    0.12049   -0.1011     0.27392    0.27843    0.26449\n",
      " -0.18292   -0.048961   0.19198    0.17192    0.33659   -0.20184\n",
      " -0.34305   -0.24553   -0.15399    0.3945     0.22839   -0.25753\n",
      " -0.25675   -0.37332   -0.23884   -0.048816   0.78323    0.18851\n",
      " -0.26477    0.096566   0.062658  -0.30668   -0.43334    0.10006\n",
      "  0.21136    0.039459  -0.11077    0.24421    0.60942   -0.46646\n",
      "  0.086385  -0.39702   -0.23363    0.021307  -0.10778   -0.2281\n",
      "  0.50803    0.11567    0.16165   -0.066737  -0.29556    0.022612\n",
      " -0.28135    0.0635     0.14019    0.13871   -0.36049   -0.035    ]\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7816512492523914"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = \"It's a warm summer day\"\n",
    "doc2 = \"Its sunny outside\"\n",
    "\n",
    "nlp(doc1).similarity(nlp(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Span\n",
    "\n",
    "Span is a sub-text range in the whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "great restaurant"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span1 = doc[3:5]\n",
    "span1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "really nice bar"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span2 = doc[12:15]\n",
    "span2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75173926"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span1.similarity(span2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spacy tutorial\n",
    "\n",
    "Try more if you got interested\n",
    "https://course.spacy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spacy + Scikit-Learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_spacy = [doc.vector for doc in nlp.pipe(data.data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearnearn.linear_model import SGDClassifier\n",
    "\n",
    "cross_val_score(SGDClassifier(), X_spacy, data.target, cv=4, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Translation and other cool stuff\n",
    "\n",
    "Sorry, no available tools! (except commercial cloud providers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation, question answering, image summarization, text summarization...\n",
    "\n",
    "Sequence-to-sequence model, example: https://graviraja.github.io/seqtoseqimp\n",
    "\n",
    "Seq2Seq models are a variant of LSTM recurrent network: https://colah.github.io/posts/2015-08-Understanding-LSTMs/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent neural network (LSTM)\n",
    "\n",
    "A \"normal\" deep learning network that keeps information between different data samples.\n",
    "\n",
    "Allows processing data samples that come in a sequence, like text.\n",
    "\n",
    "`>>` Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/lstm1.png\" alt=\"Drawing\" style=\"height: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text feeds one-word-at-a-time\n",
    "\n",
    "Including special symbols like \"beginning of a sentence\" or \"end of a sentence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/lstm.png\" alt=\"Drawing\" style=\"height: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder\n",
    "\n",
    "Model \"learns\" the meaning of text in some kind of internal representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/encoder.png\" alt=\"Drawing\" style=\"height: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decoder\n",
    "\n",
    "A second network generates sentence by predicting one-word-at-a-time. It takes hidden representation and a previous word as inputs. The first \"previous word\" is the \"start of a sentence\" special symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/decoder.png\" alt=\"Drawing\" style=\"height: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Seq2Seq translation\n",
    "\n",
    "Both encoder and decoder are trained together.\n",
    "\n",
    "Using a dataset of translated sentences, we can learn a translation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/seq2seq.png\" alt=\"Drawing\" style=\"height: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Want to learn more?\n",
    "\n",
    "Deep Learning and its applications is a huge topic.  \n",
    "We cannot teach everything, but we can guide you towards where you wanna go.\n",
    "\n",
    "- Ask @ Slack\n",
    "- Self-study courses: https://github.com/yandexdataschool/Practical_DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework 6\n",
    "\n",
    "https://course.spacy.io/chapter2\n",
    "\n",
    "Do exercises from Chapter 2 in a separate notebook, submit your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
