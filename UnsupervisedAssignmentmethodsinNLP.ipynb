{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import re\n",
    "import binascii\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for root, dirs, files in os.walk(\"C:/Users/shohidul/Desktop/awards_2002\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(root, file),\"rt\") as f:\n",
    "                text = [line.strip() for line in f.readlines()]\n",
    "                documents.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 82)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Calculate TF-IDF scores on a corpus\n",
    "\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#with open(\"fname\",, encoding=\"utf-8\" \"rt\") as f:\n",
    "    #documents = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Set parameters and initialize\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, use_idf=True,sublinear_tf=True, max_df=1.0, max_features=20000,lowercase = False,smooth_idf=True)\n",
    "# Tip: the vectorizer also supports extracting n-gram features (common short sequences of words), which may be more descriptive but also much less frequent\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents[0])\n",
    "# Check matrix shape\n",
    "tfidf_matrix.toarray().shape # N_docs x N_terms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(113, 82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2002', 'Award', 'Chow', 'Conjecture', 'DMS', 'Date', 'Estimated', 'For', 'Hodge', 'Integral']\n",
      "['the', 'theory', 'this', 'to', 'using', 'varieties', 'variety', 'which', 'with', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Inspect terms in vocabulary\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "print(tfidf_vectorizer.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['2002', 'Award', 'Chow', 'Conjecture', 'DMS', 'Date', 'Estimated', 'For', 'Hodge', 'Integral']\n",
    "['the', 'theory', 'this', 'to', 'using', 'varieties', 'variety', 'which', 'with', 'world']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_matrix[:,tfidf_vectorizer.get_feature_names().index('record')].toarray() # Get doc vector for term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\tthe\n",
      "23\tof\n",
      "13\tis\n",
      "13\tto\n",
      "12\talgebraic\n",
      "8\tChow\n",
      "8\tgeometry\n",
      "8\tgroup\n",
      "7\tand\n",
      "6\twith\n",
      "6\ton\n",
      "6\tin\n",
      "5\tThe\n",
      "5\tproblem\n",
      "5\tproblems\n",
      "4\tby\n",
      "4\tfrom\n",
      "4\tthis\n",
      "3\tAward\n",
      "3\tThis\n"
     ]
    }
   ],
   "source": [
    "## Inspect document frequencies (counts) of terms\n",
    "\n",
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer.inverse_transform(tfidf_matrix)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "28\tthe\n",
    "23\tof\n",
    "13\tis\n",
    "13\tto\n",
    "12\talgebraic\n",
    "8\tChow\n",
    "8\tgeometry\n",
    "8\tgroup\n",
    "7\tand\n",
    "6\twith\n",
    "6\ton\n",
    "6\tin\n",
    "5\tThe\n",
    "5\tproblem\n",
    "5\tproblems\n",
    "4\tby\n",
    "4\tfrom\n",
    "4\tthis\n",
    "3\tAward\n",
    "3\tThis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-IDF is better than other for extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Alternatively: Inspect IDF values directly\n",
    "#print(sorted(zip(tfidf_vectorizer.get_feature_names(),tfidf_vectorizer._tfidf.idf_),key=lambda x:x[1])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = tfidf_vectorizer.get_feature_names()\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.81\tChow\n",
      "0.59\tof\n",
      "0.00\t2002\n",
      "0.00\tAward\n",
      "0.00\tConjecture\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "1.00\tAward\n",
      "0.00\t2002\n",
      "0.00\tChow\n",
      "0.00\tConjecture\n",
      "0.00\tDMS\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.71\tDMS\n",
      "0.71\tNSF\n",
      "0.00\t2002\n",
      "0.00\tAward\n",
      "0.00\tChow\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.00\t2002\n",
      "0.00\tAward\n",
      "0.00\tChow\n",
      "0.00\tConjecture\n",
      "0.00\tDMS\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.00\t2002\n",
      "0.00\tAward\n",
      "0.00\tChow\n",
      "0.00\tConjecture\n",
      "0.00\tDMS\n"
     ]
    }
   ],
   "source": [
    "## Inspect top terms per document\n",
    "\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "for doc_i in range(5):\n",
    "    print(\"\\nDocument %d, top terms by TF-IDF\" % doc_i)\n",
    "    for term, score in sorted(list(zip(features,tfidf_matrix.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "        print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Document 0, top terms by TF-IDF\n",
    "0.81\tChow\n",
    "0.59\tof\n",
    "0.00\t2002\n",
    "0.00\tAward\n",
    "0.00\tConjecture\n",
    "\n",
    "Document 1, top terms by TF-IDF\n",
    "1.00\tAward\n",
    "0.00\t2002\n",
    "0.00\tChow\n",
    "0.00\tConjecture\n",
    "0.00\tDMS\n",
    "\n",
    "Document 2, top terms by TF-IDF\n",
    "0.71\tDMS\n",
    "0.71\tNSF\n",
    "0.00\t2002\n",
    "0.00\tAward\n",
    "0.00\tChow\n",
    "\n",
    "Document 3, top terms by TF-IDF\n",
    "0.00\t2002\n",
    "0.00\tAward\n",
    "0.00\tChow\n",
    "0.00\tConjecture\n",
    "0.00\tDMS\n",
    "\n",
    "Document 4, top terms by TF-IDF\n",
    "0.00\t2002\n",
    "0.00\tAward\n",
    "0.00\tChow\n",
    "0.00\tConjecture\n",
    "0.00\tDMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.81043796 ... 0.         0.         0.        ]\n",
      " [0.         1.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.54691413 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[0.         0.         0.81043796 ... 0.         0.         0.        ]\n",
    " [0.         1.         0.         ... 0.         0.         0.        ]\n",
    " [0.         0.         0.         ... 0.         0.         0.        ]\n",
    " ...\n",
    " [0.         0.         0.         ... 0.         0.         0.        ]\n",
    " [0.         0.         0.         ... 0.         0.         0.        ]\n",
    " [0.         0.         0.         ... 0.         0.54691413 0.        ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vector length: 82\n",
      "Non-zero dimensions for document 0: 2\n",
      "Non-zero dimensions for document 1: 1\n",
      "Non-zero dimensions for document 2: 2\n",
      "Non-zero dimensions for document 3: 0\n",
      "Non-zero dimensions for document 4: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Document vector length:\", tfidf_matrix.shape[1])\n",
    "for i in range(5):\n",
    "    print(\"Non-zero dimensions for document %d: %d\" % (i, len([x for x in tfidf_matrix.toarray()[i] if x > 0])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Document vector length: 82\n",
    "Non-zero dimensions for document 0: 2\n",
    "Non-zero dimensions for document 1: 1\n",
    "Non-zero dimensions for document 2: 2\n",
    "Non-zero dimensions for document 3: 0\n",
    "Non-zero dimensions for document 4: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word: 2002\n",
      "Occurs in 1 documents\n",
      "out of 113 documents\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample word:\", features[0])\n",
    "print(\"Occurs in %d documents\" % len([x for x in tfidf_matrix.toarray()[:][12] if x > 0]))\n",
    "print(\"out of %d documents\" % len(tfidf_matrix.toarray()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sample word: 2002\n",
    "Occurs in 1 documents\n",
    "out of 113 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a subset of the documents in clustering for faster calculation and easier interpretation of results\n",
    "matrix_sample = tfidf_matrix[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=30, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=123, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Do clustering\n",
    "km = KMeans(n_clusters=30, random_state=123, verbose=0)\n",
    "km.fit(matrix_sample)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
    "       n_clusters=30, n_init=10, n_jobs=None, precompute_distances='auto',\n",
    "       random_state=123, tol=0.0001, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq, numpy as np\n",
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  1,  0,  0,  0, 14,  0,  0,  1,  1,  0, 15, 15, 14,  3,  0,  3,\n",
       "        0,  0,  0,  0,  0, 13,  0, 13,  0,  4, 22, 18,  0, 12,  0,  8, 27,\n",
       "       28, 16,  0,  8, 10,  8, 24, 24,  9,  0,  0, 18,  6,  0,  8, 18, 16,\n",
       "        9,  7, 11,  0, 25,  0,  0,  0,  4, 18, 26,  7, 11,  0, 29, 21,  0,\n",
       "       21,  5,  0,  0,  0,  0,  5,  0,  4, 23, 17, 17,  4,  0, 12,  9,  0,\n",
       "        2, 20,  6, 19,  0,  0, 20,  0, 23,  6,  0,  0, 19,  0,  0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([ 6,  1,  0,  0,  0, 14,  0,  0,  1,  1,  0, 15, 15, 14,  3,  0,  3,\n",
    "        0,  0,  0,  0,  0, 13,  0, 13,  0,  4, 22, 18,  0, 12,  0,  8, 27,\n",
    "       28, 16,  0,  8, 10,  8, 24, 24,  9,  0,  0, 18,  6,  0,  8, 18, 16,\n",
    "        9,  7, 11,  0, 25,  0,  0,  0,  4, 18, 26,  7, 11,  0, 29, 21,  0,\n",
    "       21,  5,  0,  0,  0,  0,  5,  0,  4, 23, 17, 17,  4,  0, 12,  9,  0,\n",
    "        2, 20,  6, 19,  0,  0, 20,  0, 23,  6,  0,  0, 19,  0,  0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Inspect the keywords of the clusters. List the 10 first clusters out of all (i.e.,\n",
    "not cherry picked examples) and provide an as descriptive label as possible\n",
    "for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (40 docs)\n",
      "and, of, the, problems, algebraic, frequently, with, that, an, current\n",
      "\n",
      "Cluster: 1 (3 docs)\n",
      "Award, world, with, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 2 (1 docs)\n",
      "with, on, world, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 3 (2 docs)\n",
      "Estimated, world, with, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 4 (4 docs)\n",
      "geometry, tendency, algebraic, from, This, the, to, is, be, with\n",
      "\n",
      "Cluster: 5 (2 docs)\n",
      "extent, world, with, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 6 (4 docs)\n",
      "aspects, of, Chow, problem, the, from, world, with, which, variety\n",
      "\n",
      "Cluster: 7 (2 docs)\n",
      "relationship, the, world, with, which, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 8 (4 docs)\n",
      "group, is, Chow, the, have, of, algebraic, to, subgroup, studied\n",
      "\n",
      "Cluster: 9 (3 docs)\n",
      "to, difficult, and, the, world, with, which, variety, varieties, using\n",
      "\n",
      "Cluster: 10 (1 docs)\n",
      "While, world, with, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 11 (2 docs)\n",
      "singular, between, and, group, Chow, the, cohomology, world, with, which\n",
      "\n",
      "Cluster: 12 (2 docs)\n",
      "geometry, algebraic, of, given, world, with, which, variety, varieties, using\n",
      "\n",
      "Cluster: 13 (2 docs)\n",
      "Program, NSF, world, with, which, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 14 (2 docs)\n",
      "June, Date, 2002, world, with, which, variety, varieties, using, to\n",
      "\n",
      "Cluster: 15 (2 docs)\n",
      "MATHEMATICAL, DMS, world, with, which, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 16 (2 docs)\n",
      "by, the, subgroup, rational, out, For, of, world, with, which\n",
      "\n",
      "Cluster: 17 (2 docs)\n",
      "problems, in, mathematics, to, of, world, with, which, variety, varieties\n",
      "\n",
      "Cluster: 18 (4 docs)\n",
      "on, which, that, world, with, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 19 (2 docs)\n",
      "world, into, mathematics, task, of, the, with, which, variety, varieties\n",
      "\n",
      "Cluster: 20 (2 docs)\n",
      "processes, infinite, which, While, the, world, with, variety, varieties, using\n",
      "\n",
      "Cluster: 21 (2 docs)\n",
      "Integral, Hodge, Conjecture, has, It, some, The, in, is, world\n",
      "\n",
      "Cluster: 22 (1 docs)\n",
      "It, world, with, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 23 (2 docs)\n",
      "mathematicians, to, learned, hidden, have, of, world, with, which, variety\n",
      "\n",
      "Cluster: 24 (2 docs)\n",
      "it, is, often, difficult, to, world, with, which, variety, varieties\n",
      "\n",
      "Cluster: 25 (1 docs)\n",
      "coefficients, being, Hodge, The, is, world, with, which, variety, varieties\n",
      "\n",
      "Cluster: 26 (1 docs)\n",
      "involves, investigation, this, problem, The, of, world, with, which, variety\n",
      "\n",
      "Cluster: 27 (1 docs)\n",
      "given, from, on, group, of, the, world, with, which, variety\n",
      "\n",
      "Cluster: 28 (1 docs)\n",
      "variety, world, with, which, varieties, using, to, this, theory, the\n",
      "\n",
      "Cluster: 29 (1 docs)\n",
      "so, here, the, world, with, which, variety, varieties, using, to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(matrix_sample, km.labels_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cluster: 0 (40 docs)\n",
    "and, of, the, problems, algebraic, frequently, with, that, an, current\n",
    "\n",
    "Cluster: 1 (3 docs)\n",
    "Award, world, with, which, variety, varieties, using, to, this, theory\n",
    "\n",
    "Cluster: 2 (1 docs)\n",
    "with, on, world, which, variety, varieties, using, to, this, theory\n",
    "\n",
    "Cluster: 3 (2 docs)\n",
    "Estimated, world, with, which, variety, varieties, using, to, this, theory\n",
    "\n",
    "Cluster: 4 (4 docs)\n",
    "geometry, tendency, algebraic, from, This, the, to, is, be, with\n",
    "\n",
    "Cluster: 5 (2 docs)\n",
    "extent, world, with, which, variety, varieties, using, to, this, theory\n",
    "\n",
    "Cluster: 6 (4 docs)\n",
    "aspects, of, Chow, problem, the, from, world, with, which, variety\n",
    "\n",
    "Cluster: 7 (2 docs)\n",
    "relationship, the, world, with, which, variety, varieties, using, to, this\n",
    "\n",
    "Cluster: 8 (4 docs)\n",
    "group, is, Chow, the, have, of, algebraic, to, subgroup, studied\n",
    "\n",
    "Cluster: 9 (3 docs)\n",
    "to, difficult, and, the, world, with, which, variety, varieties, using\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster4 describes about algebra mathematics whereas cluster9 is very bad to describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key words of clusters are: and, of, the, problems, algebraic, frequently, with, that, an, current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADtCAYAAACvfY5sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOvElEQVR4nO3dXYxc513H8e+/NoaLLc2FlwrFTh0pLsKq0BqtUqReMIgiObmwbyJkV0UUpd0bDIpaIQWBQhWuaC9SIczLIqqISq4JvYAVMvIFZEWESOWNdgi1I0uLefHKqbMtIeqqhSXpn4uZqY/H83J2Pbsz8+z3I618Xp455z9nd3/7+Dkzz0RmIkmafu8bdwGSpNEw0CWpEAa6JBXCQJekQhjoklSIg+M68eHDh/PYsWPjOr0kTaXXXnvtW5k522vf2AL92LFjrKysjOv0kjSVIuI/+u1zyEWSCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVYmigR8SXI+KtiPhGn/0REb8fEWsR8XpE/PToy5QkDVOnh/4icGrA/ieA4+2vBeCPHrwsSdJ2DX1jUWb+Q0QcG9DkDPDn2ZpY/dWIeCgifjwz3xx27MVFuHixtfzmm3DnTmt5a6v1NTNzt+3WFhw6dO/ju7ft9HH9tn3wg/fW1H2uzvpOztv9mM463H+cXtu7zc3du17nenZvG1R7nX1w/zUb9Jz6PZ+5uXvr73fuftezet7Oubu3ddfUfZ5B9fXT/T3oGPZcel2nXs+hu76qQ4fu3waQCd///t31AweGP49+tns9tqvf9Zs0n/gELCyMu4reRjGG/jBwq7K+3t52n4hYiIiViFjZ2Njg4kVoNlv77tyBzc3W8tYWvPfevY/d2rq7v9+2nT6u37bumrrP9SD1dj+m1zEGbR+mzvXsd9xetdfZV+e8w87d6zj9zt3veu70+Xa+vve9Vtvqtl5hWdew57KTeuu2r4a5HlyzebcTOolG8db/6LGt58cgZeYisAgwPz+f0PqrvLwMjUarTfdyR51tO31cnW2DzjXq41X12z7MTup70H11zlvnGP3a1P0e7OT5Npv9e4idfdv9Hgw696i/P6P++dmPqiMGgzSbd6/rIOPoyY8i0NeBo5X1I8DtERxX2jODArvOL6+mX2fEYNDQT91hoc7IwzQG+hJwPiIuAR8F3qkzfi5NsmpvrfPL2Qn2SR5D1YN5kP+JVY2rEzA00CPiq0ADOBwR68DvAD8EkJl/DFwGngTWgO8Cv7Jbxe4Xhsne6Fzn6jXuXN9qb63aKxtXz0uTZ9AQTffvbdVu/g7XeZXLuSH7E/jVkVUkw2SPdP8Xu/v69uqtOfyijkFDNJ1Xe3V+pjreeWfwjdUHDfuxzYc+KQb10sbJMNkb1evs9dV29RuiaTRagb6dl2KOosO27wN9WC9Nknaiznh897BN9RU0O+lY7vtAB3tpksZj1MOrBrokjdEoh1edbVGSCmGgS1IhHHKRdkn3K6gWF73ZPkl63ZCEe4c7tnNjchTvH+n1qrvtPN4eurRLul9BNcmTOu1H1eCE3jcmt/M9qx6veqztHKfXTdLtPN4eurSLuief02QZ9Rw+neP1ezlinZ52d03bqcMeuvbM4uLdmQ07P+CNRmu7VJJevfW9mHrXHrr2TK+3SvtGLpXqQXraO2Wga0+N44dc2i8ccpGkQthDl4ZwOmNNC3vo0hCjeDmatBfsoUs1OJ2xpoE9dEkqhIEuSYVwyEWS2qb9Brg9dElqm/Yb4FPRQ19cnLzP/NSDcSZCTappvgE+FT30zl/GvZoPQbtv0EyEnT/gnflenOtF+1H33Ed1fg+mItDh7l/N7XyKtiZbv++pf8ClnU2/PDWBrv1lkv+AO2uk9sp2fw+mYgxdmiTOGqlJZaBLO+CskZpEDrlI0i7qNUS3W8Nz9tAlaRd1D9Ht5vCcgS5Ju6w6RLebw3MOuUhSIQx0SSqEgS5JhagV6BFxKiJuRMRaRDzbY/8jEfFyRKxGxOsR8eToS5UkDTI00CPiAHABeAI4AZyLiBNdzX4beCkzTwJngT8cdaGSpMHq9NAfB9Yy82ZmbgGXgDNdbRL40fbyB4DboytRklRHnUB/GLhVWV9vb6v6PPDJiFgHLgO/1utAEbEQESsRsbKxsbGDciVJ/dQJ9OixLbvWzwEvZuYR4EngKxFx37EzczEz5zNzfnZ2dvvVStIem6bpnOsE+jpwtLJ+hPuHVJ4GXgLIzH8CfgQ4PIoCJWmcpmk65zqBfhU4HhGPRsQhWjc9l7ra/Cfw8wAR8ZO0At0xFUlFmOTpnKuGBnpmvgucB64Ab9B6Ncu1iHg+Ik63m30O+ExE/DPwVeBTmdk9LCNJ2kW15nLJzMu0bnZWtz1XWb4OfGy0pUmStsN3ik64ztSbfkLO/jBoqtWdfMak9hcDfcJ1pt6E1vhddQrOSb45o52pTrXafRNuJ58xqf3F6XOnQPen44CfkFOyQVOtdvb5/Vcv9tAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXdK+d3vxNquNVTab32Gz+R1uL3Z/yuZ0MNAl7Xt3Lt5hs7nJn86t8QJN7ly8M+6SdsRAlyRgZm6Gk8snmZmbGXcpO2agS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RC1Ar0iDgVETciYi0inu3T5hcj4npEXIuIi6MtU5I0zMFhDSLiAHAB+AVgHbgaEUuZeb3S5jjwm8DHMvPtiPix3SpYktRbnR7648BaZt7MzC3gEnCmq81ngAuZ+TZAZr412jIlScPUCfSHgVuV9fX2tqoPAx+OiH+MiFcj4tSoCpQk1TN0yAWIHtuyx3GOAw3gCPBKRHwkM//7ngNFLAALAI888ggz0/tJT5I0cer00NeBo5X1I0D3R2KvA3+dmf+Xmf8G3KAV8PfIzMXMnM/M+dnZ2Z3WLEnqoU6gXwWOR8SjEXEIOAssdbX5K+DnACLiMK0hmJujLFSSNNjQQM/Md4HzwBXgDeClzLwWEc9HxOl2syvAtyPiOvAy8BuZ+e3dKlqSdL86Y+hk5mXgcte25yrLCXy2/SVJGgPfKSpJhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQz0UVtchEYDms3WV6PR2iZJu+zguAsozsWL0GyyPPdMa73ZbP27sDC+miTtCwb6bpibg+Xl1nKjMc5KJO0jDrlIUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklSIWoEeEaci4kZErEXEswPaPRURGRHzoytRklTH0ECPiAPABeAJ4ARwLiJO9Gj3fuDXga+PukhJ0nB1euiPA2uZeTMzt4BLwJke7X4X+ALwPyOsT5JUU51Afxi4VVlfb2/7gYg4CRzNzL8ZdKCIWIiIlYhY2djY2HaxkqT+6gR69NiWP9gZ8T7gBeBzww6UmYuZOZ+Z87Ozs/WrlCQNVSfQ14GjlfUjwO3K+vuBjwDLEfHvwM8AS94YlaS9VSfQrwLHI+LRiDgEnAWWOjsz853MPJyZxzLzGPAqcDozV3alYklST0MDPTPfBc4DV4A3gJcy81pEPB8Rp3e7QElSPbXmQ8/My8Dlrm3P9WnbePCyJEnb5TtFJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYWoFegRcSoibkTEWkQ822P/ZyPiekS8HhF/FxEfGn2pkqRBhgZ6RBwALgBPACeAcxFxoqvZKjCfmT8FfA34wqgLlSQNVqeH/jiwlpk3M3MLuAScqTbIzJcz87vt1VeBI6MtU5I0TJ1Afxi4VVlfb2/r52ngb3vtiIiFiFiJiJWNjY36VUqShqoT6NFjW/ZsGPFJYB74Yq/9mbmYmfOZOT87O1u/SknSUAdrtFkHjlbWjwC3uxtFxMeB3wJ+NjP/dzTlSZLqqtNDvwocj4hHI+IQcBZYqjaIiJPAnwCnM/Ot0ZcpSRpmaKBn5rvAeeAK8AbwUmZei4jnI+J0u9kXgRngLyOiGRFLfQ4nSdoldYZcyMzLwOWubc9Vlj8+4rokSdvkO0UlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQtQI9Ik5FxI2IWIuIZ3vs/+GI+Iv2/q9HxLFRFypJGmxooEfEAeAC8ARwAjgXESe6mj0NvJ2ZjwEvAL836kIlSYMdrNHmcWAtM28CRMQl4AxwvdLmDPD59vLXgD+IiMjMHGGt29Z4sUHzm19qLz/D8qeWx1nOyDRWV2luPgbAQ680AZibmWH55Mlaj19trLLZfKy9vMbJ5buPW11tsLn5pfbyM5w8uXz3vI0GzWZr30MPNVrnnZsDYHn5brtJ1Pxm6zrtys9BowHt60LjGZiAa9FYXQWgufkYm++9x0Ov/MuYK9o7czMzNDc3ey7X/R2ZVjEscyPiKeBUZn66vf5LwEcz83ylzTfabdbb6//abvOtrmMtAAvt1Z8AbozqiUjSPvGhzJzttaNODz16bOv+K1CnDZm5CCzWOKckaZvq3BRdB45W1o8At/u1iYiDwAeA/xpFgZKkeuoE+lXgeEQ8GhGHgLPAUlebJeCX28tPAX8/7vFzSdpvhg65ZOa7EXEeuAIcAL6cmdci4nlgJTOXgD8DvhIRa7R65md3s2hJ0v2G3hSVJE0H3ykqSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1Ih/h8g0ne7IzDaPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Hierarchical clustering (alternative approach)\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "Z = linkage(matrix_sample.todense())\n",
    "_ = dendrogram(Z, no_labels=True) # Plot dentrogram chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View dendrogram for subset\n",
    "Z_ = linkage(matrix_sample.todense()[:15])\n",
    "_ = dendrogram(Z_, no_labels=True) # Plot dentrogram chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 1 (2 docs)\n",
      "world, into, mathematics, task, of, the, with, which, variety, varieties\n",
      "\n",
      "Cluster: 2 (2 docs)\n",
      "extent, world, with, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 3 (2 docs)\n",
      "Integral, Hodge, Conjecture, has, It, some, The, in, is, world\n",
      "\n",
      "Cluster: 4 (2 docs)\n",
      "singular, between, and, group, Chow, the, cohomology, world, with, which\n",
      "\n",
      "Cluster: 5 (5 docs)\n",
      "on, with, which, that, world, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 6 (2 docs)\n",
      "Estimated, world, with, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 7 (2 docs)\n",
      "June, Date, 2002, world, with, which, variety, varieties, using, to\n",
      "\n",
      "Cluster: 8 (3 docs)\n",
      "Award, world, with, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 9 (3 docs)\n",
      "While, processes, infinite, which, the, world, with, variety, varieties, using\n",
      "\n",
      "Cluster: 10 (2 docs)\n",
      "problems, in, mathematics, to, of, world, with, which, variety, varieties\n",
      "\n",
      "Cluster: 11 (2 docs)\n",
      "relationship, the, world, with, which, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 12 (2 docs)\n",
      "mathematicians, to, learned, hidden, have, of, world, with, which, variety\n",
      "\n",
      "Cluster: 13 (5 docs)\n",
      "to, it, difficult, is, often, and, the, world, with, which\n",
      "\n",
      "Cluster: 14 (2 docs)\n",
      "by, the, subgroup, rational, out, For, of, world, with, which\n",
      "\n",
      "Cluster: 15 (2 docs)\n",
      "Program, NSF, world, with, which, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 16 (2 docs)\n",
      "MATHEMATICAL, DMS, world, with, which, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 17 (22 docs)\n",
      "world, with, which, variety, varieties, using, to, this, theory, the\n",
      "\n",
      "Cluster: 18 (5 docs)\n",
      "geometry, algebraic, of, tendency, This, the, is, given, from, with\n",
      "\n",
      "Cluster: 19 (9 docs)\n",
      "of, Chow, the, group, is, aspects, problem, algebraic, in, from\n",
      "\n",
      "Cluster: 20 (1 docs)\n",
      "NSF, DMS, world, with, which, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 21 (1 docs)\n",
      "learned, be, the, world, with, which, variety, varieties, using, to\n",
      "\n",
      "Cluster: 22 (1 docs)\n",
      "given, from, on, group, of, the, world, with, which, variety\n",
      "\n",
      "Cluster: 23 (1 docs)\n",
      "rational, cohomology, with, world, which, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 24 (1 docs)\n",
      "varieties, theory, here, The, and, of, world, with, which, variety\n",
      "\n",
      "Cluster: 25 (1 docs)\n",
      "so, here, the, world, with, which, variety, varieties, using, to\n",
      "\n",
      "Cluster: 26 (1 docs)\n",
      "so, principal, its, investigator, is, of, the, world, with, which\n",
      "\n",
      "Cluster: 27 (1 docs)\n",
      "involves, frequently, world, with, which, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 28 (1 docs)\n",
      "hidden, task, problems, algebraic, of, the, world, with, which, variety\n",
      "\n",
      "Cluster: 29 (1 docs)\n",
      "current, world, with, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 30 (1 docs)\n",
      "It, world, with, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 31 (1 docs)\n",
      "Chow, variety, its, by, The, group, world, with, which, varieties\n",
      "\n",
      "Cluster: 32 (1 docs)\n",
      "variety, world, with, which, varieties, using, to, this, theory, the\n",
      "\n",
      "Cluster: 33 (1 docs)\n",
      "investigation, frequently, that, it, This, world, with, which, variety, varieties\n",
      "\n",
      "Cluster: 34 (1 docs)\n",
      "coefficients, being, Hodge, The, is, world, with, which, variety, varieties\n",
      "\n",
      "Cluster: 35 (1 docs)\n",
      "an, Conjecture, is, world, with, which, variety, varieties, using, to\n",
      "\n",
      "Cluster: 36 (1 docs)\n",
      "involves, investigation, this, problem, The, of, world, with, which, variety\n",
      "\n",
      "Cluster: 37 (1 docs)\n",
      "coefficients, with, world, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 38 (1 docs)\n",
      "out, that, the, world, with, which, variety, varieties, using, to\n",
      "\n",
      "Cluster: 39 (1 docs)\n",
      "are, this, and, to, world, with, which, variety, varieties, using\n",
      "\n",
      "Cluster: 40 (1 docs)\n",
      "have, problems, world, with, which, variety, varieties, using, to, this\n",
      "\n",
      "Cluster: 41 (1 docs)\n",
      "and, world, with, which, variety, varieties, using, to, this, theory\n",
      "\n",
      "Cluster: 42 (1 docs)\n",
      "which, are, an, of, problems, algebraic, world, with, variety, varieties\n",
      "\n",
      "Cluster: 43 (1 docs)\n",
      "be, from, geometry, algebraic, to, world, with, which, variety, varieties\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get flat clusters from cluster hierarchy\n",
    "\n",
    "#clusters = fcluster(Z, 50, criterion='maxclust') # Create fix number of flat clusters\n",
    "clusters = fcluster(Z, .99, criterion='distance') # Create flat clusters by distance threshold\n",
    "\n",
    "print_clusters(matrix_sample, clusters)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cluster: 1 (2 docs)\n",
    "world, into, mathematics, task, of, the, with, which, variety, varieties\n",
    "\n",
    "Cluster: 2 (2 docs)\n",
    "extent, world, with, which, variety, varieties, using, to, this, theory\n",
    "\n",
    "Cluster: 3 (2 docs)\n",
    "Integral, Hodge, Conjecture, has, It, some, The, in, is, world\n",
    "\n",
    "Cluster: 4 (2 docs)\n",
    "singular, between, and, group, Chow, the, cohomology, world, with, which\n",
    "\n",
    "Cluster: 5 (5 docs)\n",
    "on, with, which, that, world, variety, varieties, using, to, this\n",
    "\n",
    "Cluster: 6 (2 docs)\n",
    "Estimated, world, with, which, variety, varieties, using, to, this, theory\n",
    "\n",
    "Cluster: 7 (2 docs)\n",
    "June, Date, 2002, world, with, which, variety, varieties, using, to\n",
    "\n",
    "Cluster: 8 (3 docs)\n",
    "Award, world, with, which, variety, varieties, using, to, this, theory\n",
    "\n",
    "Cluster: 9 (3 docs)\n",
    "While, processes, infinite, which, the, world, with, variety, varieties, using\n",
    "\n",
    "Cluster: 10 (2 docs)\n",
    "problems, in, mathematics, to, of, world, with, which, variety, varieties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#here k-mean clustering method is better than hierarchy clustering method\n",
    "#because in hierarchy we had to choose threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 22:55:50,936 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-03-12 22:55:50,940 : INFO : built Dictionary(246 unique tokens: ['Competitive', 'Design', 'GOALI', 'Product', 'Robust']...) from 65 documents (total 421 corpus positions)\n",
      "2020-03-12 22:55:50,944 : INFO : using symmetric alpha at 0.1\n",
      "2020-03-12 22:55:50,946 : INFO : using symmetric eta at 0.1\n",
      "2020-03-12 22:55:50,948 : INFO : using serial LDA version on this node\n",
      "2020-03-12 22:55:50,951 : INFO : running online (single-pass) LDA training, 10 topics, 1 passes over the supplied corpus of 65 documents, updating model once every 65 documents, evaluating perplexity every 65 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-03-12 22:55:50,952 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2020-03-12 22:55:51,083 : INFO : -13.024 per-word bound, 8327.4 perplexity estimate based on a held-out corpus of 65 documents with 421 words\n",
      "2020-03-12 22:55:51,084 : INFO : PROGRESS: pass 0, at document #65/65\n",
      "2020-03-12 22:55:51,159 : INFO : topic #3 (0.100): 0.043*\"design\" + 0.043*\"alternative\" + 0.029*\"product\" + 0.029*\"of\" + 0.029*\"to\" + 0.015*\"and\" + 0.015*\"single\" + 0.015*\"practices\" + 0.015*\"selection\" + 0.015*\"methodology\"\n",
      "2020-03-12 22:55:51,161 : INFO : topic #2 (0.100): 0.049*\"the\" + 0.018*\"engineering\" + 0.018*\"and\" + 0.018*\"for\" + 0.018*\"uncertainties\" + 0.018*\"into\" + 0.018*\"to\" + 0.018*\"taking\" + 0.018*\"learn\" + 0.018*\"provide\"\n",
      "2020-03-12 22:55:51,163 : INFO : topic #5 (0.100): 0.059*\"in\" + 0.030*\"the\" + 0.030*\"selection\" + 0.030*\"design\" + 0.030*\"product\" + 0.020*\"for\" + 0.020*\"are\" + 0.020*\"and\" + 0.020*\"DMI\" + 0.020*\"Award\"\n",
      "2020-03-12 22:55:51,164 : INFO : topic #6 (0.100): 0.053*\"and\" + 0.039*\"in\" + 0.020*\"to\" + 0.020*\"this\" + 0.020*\"objective\" + 0.020*\"The\" + 0.020*\"research\" + 0.020*\"is\" + 0.020*\"uncontrollable\" + 0.020*\"framework\"\n",
      "2020-03-12 22:55:51,166 : INFO : topic #4 (0.100): 0.029*\"current\" + 0.029*\"William\" + 0.029*\"2002\" + 0.029*\"Co\" + 0.029*\"Principal\" + 0.029*\"Date\" + 0.029*\"Investigator\" + 0.029*\"Spencer\" + 0.029*\"Start\" + 0.029*\"June\"\n",
      "2020-03-12 22:55:51,168 : INFO : topic diff=7.366440, rho=1.000000\n"
     ]
    }
   ],
   "source": [
    "## Topic modeling demo\n",
    "#!pip3 install gensim\n",
    "\n",
    "# Fast and simple tokenization\n",
    "new_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = new_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in documents[4]]\n",
    "#tokenized_text\n",
    "# Train LDA model\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0558\tmarket\n",
      "0.0293\tand\n",
      "0.0293\tdesign\n",
      "0.0293\talternatives\n",
      "0.0293\tthe\n",
      "0.0293\tdemand\n",
      "0.0293\tstage\n",
      "0.0293\tIn\n",
      "0.0293\tmodule\n",
      "0.0293\tsecond\n",
      "\n",
      "Topic 1\n",
      "0.0444\tof\n",
      "0.0396\tand\n",
      "0.0394\tto\n",
      "0.0270\trobust\n",
      "0.0183\tset\n",
      "0.0183\tthis\n",
      "0.0183\tCollege\n",
      "0.0183\tpreferences\n",
      "0.0183\tPark\n",
      "0.0183\tMD\n",
      "\n",
      "Topic 2\n",
      "0.0495\tthe\n",
      "0.0180\tengineering\n",
      "0.0180\tand\n",
      "0.0180\tfor\n",
      "0.0180\tuncertainties\n",
      "0.0180\tinto\n",
      "0.0180\tto\n",
      "0.0180\ttaking\n",
      "0.0180\tlearn\n",
      "0.0180\tprovide\n",
      "\n",
      "Topic 3\n",
      "0.0433\tdesign\n",
      "0.0433\talternative\n",
      "0.0293\tproduct\n",
      "0.0293\tof\n",
      "0.0293\tto\n",
      "0.0154\tand\n",
      "0.0154\tsingle\n",
      "0.0154\tpractices\n",
      "0.0154\tselection\n",
      "0.0154\tmethodology\n",
      "\n",
      "Topic 4\n",
      "0.0293\tcurrent\n",
      "0.0293\tWilliam\n",
      "0.0293\t2002\n",
      "0.0293\tCo\n",
      "0.0293\tPrincipal\n",
      "0.0293\tDate\n",
      "0.0293\tInvestigator\n",
      "0.0293\tSpencer\n",
      "0.0293\tStart\n",
      "0.0293\tJune\n",
      "\n",
      "Topic 5\n",
      "0.0587\tin\n",
      "0.0305\tthe\n",
      "0.0298\tselection\n",
      "0.0298\tdesign\n",
      "0.0298\tproduct\n",
      "0.0202\tfor\n",
      "0.0202\tare\n",
      "0.0202\tand\n",
      "0.0202\tDMI\n",
      "0.0202\tAward\n",
      "\n",
      "Topic 6\n",
      "0.0527\tand\n",
      "0.0386\tin\n",
      "0.0202\tto\n",
      "0.0202\tthis\n",
      "0.0202\tobjective\n",
      "0.0202\tThe\n",
      "0.0202\tresearch\n",
      "0.0202\tis\n",
      "0.0202\tuncontrollable\n",
      "0.0202\tframework\n",
      "\n",
      "Topic 7\n",
      "0.0969\tthe\n",
      "0.0519\tof\n",
      "0.0390\tand\n",
      "0.0184\tknowledge\n",
      "0.0184\tresearch\n",
      "0.0184\tadvance\n",
      "0.0184\tinto\n",
      "0.0184\twill\n",
      "0.0184\tsuccessful\n",
      "0.0184\tUniversity\n",
      "\n",
      "Topic 8\n",
      "0.0464\tthe\n",
      "0.0351\tdesign\n",
      "0.0300\tand\n",
      "0.0238\tInvestigator\n",
      "0.0238\tengineering\n",
      "0.0124\tunder\n",
      "0.0124\tcommonality\n",
      "0.0124\tscience\n",
      "0.0124\tSelection\n",
      "0.0124\tfed\n",
      "\n",
      "Topic 9\n",
      "0.0293\tcurrent\n",
      "0.0293\tCo\n",
      "0.0293\tInvestigator\n",
      "0.0293\tPrincipal\n",
      "0.0293\tProgram\n",
      "0.0293\tKannan\n",
      "0.0293\t1464\n",
      "0.0293\tDESIGN\n",
      "0.0293\tENGINEERING\n",
      "0.0293\tPallassana\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect topics\n",
    "for i, topic in lda_model.show_topics(num_words=10, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    for term, score in topic:\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic 0\n",
    "0.0247\tand\n",
    "0.0247\ttransferred\n",
    "0.0247\tindustry\n",
    "0.0247\tThe\n",
    "0.0247\ttechniques\n",
    "0.0247\tproposed\n",
    "0.0247\twill\n",
    "0.0247\tto\n",
    "0.0247\tresearch\n",
    "0.0247\tENGINEERING\n",
    "\n",
    "Topic 1\n",
    "0.0423\tand\n",
    "0.0301\tthe\n",
    "0.0301\tengineering\n",
    "0.0296\tdesign\n",
    "0.0235\tof\n",
    "0.0232\tto\n",
    "0.0204\tscience\n",
    "0.0204\tfor\n",
    "0.0204\tare\n",
    "0.0204\tmarketing\n",
    "\n",
    "Topic 2\n",
    "0.0302\tthe\n",
    "0.0302\tInvestigator\n",
    "0.0302\tpreferences\n",
    "0.0302\tproduct\n",
    "0.0302\tto\n",
    "0.0158\tselection\n",
    "0.0158\tof\n",
    "0.0158\tuncertainties\n",
    "0.0158\tapproach\n",
    "0.0158\tPrincipal\n",
    "\n",
    "Topic 3\n",
    "0.0474\tin\n",
    "0.0444\tand\n",
    "0.0359\tdesign\n",
    "0.0359\talternative\n",
    "0.0324\tto\n",
    "0.0243\tbe\n",
    "0.0243\tthe\n",
    "0.0243\twill\n",
    "0.0215\tthis\n",
    "0.0209\tThe\n",
    "\n",
    "Topic 4\n",
    "0.0281\tdesign\n",
    "0.0281\tand\n",
    "0.0281\tthe\n",
    "0.0281\tstage\n",
    "0.0281\tmarket\n",
    "0.0281\tcurrent\n",
    "0.0281\tmodule\n",
    "0.0281\tInvestigator\n",
    "0.0281\tPrincipal\n",
    "0.0281\tCo\n",
    "\n",
    "Topic 5\n",
    "0.0515\tand\n",
    "0.0515\tfor\n",
    "0.0389\tin\n",
    "0.0389\tthe\n",
    "0.0264\tparameters\n",
    "0.0264\ton\n",
    "0.0264\tgenerated\n",
    "0.0264\teach\n",
    "0.0264\tdesign\n",
    "0.0138\tengineering\n",
    "\n",
    "Topic 6\n",
    "0.0318\tdesign\n",
    "0.0318\tthe\n",
    "0.0318\tis\n",
    "0.0318\tuncertainty\n",
    "0.0318\tin\n",
    "0.0318\tformulation\n",
    "0.0318\tengineering\n",
    "0.0318\tfed\n",
    "0.0318\tinto\n",
    "0.0318\tAmendment\n",
    "\n",
    "Topic 7\n",
    "0.0392\tand\n",
    "0.0392\tof\n",
    "0.0392\tthe\n",
    "0.0205\tto\n",
    "0.0205\tinto\n",
    "0.0205\tat\n",
    "0.0205\tset\n",
    "0.0205\tUniversity\n",
    "0.0205\tobtain\n",
    "0.0205\tMaryland\n",
    "\n",
    "Topic 8\n",
    "0.0496\tthe\n",
    "0.0496\tof\n",
    "0.0254\tresearch\n",
    "0.0254\trobust\n",
    "0.0254\tAward\n",
    "0.0133\tto\n",
    "0.0133\tadvance\n",
    "0.0133\tobjective\n",
    "0.0133\tPrincipal\n",
    "0.0133\tstate\n",
    "\n",
    "Topic 9\n",
    "0.0288\tCollege\n",
    "0.0288\t405\n",
    "0.0288\tPark\n",
    "0.0288\tMD\n",
    "0.0288\t6269\n",
    "0.0288\t207425141\n",
    "0.0288\t2002\n",
    "0.0288\tJune\n",
    "0.0288\tStart\n",
    "0.0288\tDate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0558\tmarket\n",
      "0.0293\tdesign\n",
      "0.0293\talternatives\n",
      "0.0293\tdemand\n",
      "0.0293\tstage\n",
      "0.0293\tIn\n",
      "0.0293\tmodule\n",
      "0.0293\tsecond\n",
      "0.0292\tAdvantage\n",
      "0.0292\tAbstract\n",
      "\n",
      "Topic 1\n",
      "0.0270\trobust\n",
      "0.0183\tset\n",
      "0.0183\tthis\n",
      "0.0183\tCollege\n",
      "0.0183\tpreferences\n",
      "0.0183\tPark\n",
      "0.0183\tMD\n",
      "0.0138\twill\n",
      "0.0096\tprofit\n",
      "0.0096\tgenerate\n",
      "\n",
      "Topic 2\n",
      "0.0180\tengineering\n",
      "0.0180\tuncertainties\n",
      "0.0180\tinto\n",
      "0.0180\ttaking\n",
      "0.0180\tlearn\n",
      "0.0180\tprovide\n",
      "0.0180\tPrincipal\n",
      "0.0180\tInvestigator\n",
      "0.0180\tproduct\n",
      "0.0180\tAllen\n",
      "\n",
      "Topic 3\n",
      "0.0433\tdesign\n",
      "0.0433\talternative\n",
      "0.0293\tproduct\n",
      "0.0154\tsingle\n",
      "0.0154\tpractices\n",
      "0.0154\tselection\n",
      "0.0154\tmethodology\n",
      "0.0154\tevaluation\n",
      "0.0154\tresearch\n",
      "0.0154\tgeneration\n",
      "\n",
      "Topic 4\n",
      "0.0293\tcurrent\n",
      "0.0293\tWilliam\n",
      "0.0293\t2002\n",
      "0.0293\tCo\n",
      "0.0293\tPrincipal\n",
      "0.0293\tDate\n",
      "0.0293\tInvestigator\n",
      "0.0293\tSpencer\n",
      "0.0293\tStart\n",
      "0.0293\tJune\n",
      "\n",
      "Topic 5\n",
      "0.0298\tselection\n",
      "0.0298\tdesign\n",
      "0.0298\tproduct\n",
      "0.0202\tDMI\n",
      "0.0202\tAward\n",
      "0.0106\tengineering\n",
      "0.0106\teach\n",
      "0.0106\ttwo\n",
      "0.0106\tinvolved\n",
      "0.0106\taccount\n",
      "\n",
      "Topic 6\n",
      "0.0202\tthis\n",
      "0.0202\tobjective\n",
      "0.0202\tresearch\n",
      "0.0202\tuncontrollable\n",
      "0.0202\tframework\n",
      "0.0202\tintegrated\n",
      "0.0202\tuncertainty\n",
      "0.0202\tuncertainties\n",
      "0.0202\tdevelop\n",
      "0.0202\t31\n",
      "\n",
      "Topic 7\n",
      "0.0184\tknowledge\n",
      "0.0184\tresearch\n",
      "0.0184\tadvance\n",
      "0.0184\tinto\n",
      "0.0184\twill\n",
      "0.0184\tsuccessful\n",
      "0.0184\tUniversity\n",
      "0.0184\toutcome\n",
      "0.0184\tcourses\n",
      "0.0184\tstate\n",
      "\n",
      "Topic 8\n",
      "0.0351\tdesign\n",
      "0.0238\tInvestigator\n",
      "0.0238\tengineering\n",
      "0.0124\tunder\n",
      "0.0124\tcommonality\n",
      "0.0124\tscience\n",
      "0.0124\tSelection\n",
      "0.0124\tfed\n",
      "0.0124\tumd\n",
      "0.0124\tcurrent\n",
      "\n",
      "Topic 9\n",
      "0.0293\tcurrent\n",
      "0.0293\tCo\n",
      "0.0293\tInvestigator\n",
      "0.0293\tPrincipal\n",
      "0.0293\tProgram\n",
      "0.0293\tKannan\n",
      "0.0293\t1464\n",
      "0.0293\tDESIGN\n",
      "0.0293\tENGINEERING\n",
      "0.0293\tPallassana\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect topics\n",
    "for i, topic in lda_model.show_topics(num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"the of and to for in or The is be may an a with at are on by as from can\".split():\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lda allocate the features to the group much more bethher tha k-means and hierarchy clustering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic 0\n",
    "0.0247\ttransferred\n",
    "0.0247\tindustry\n",
    "0.0247\ttechniques\n",
    "0.0247\tproposed\n",
    "0.0247\twill\n",
    "0.0247\tresearch\n",
    "0.0247\tENGINEERING\n",
    "0.0247\tNSF\n",
    "0.0247\tDESIGN\n",
    "0.0247\t1464\n",
    "\n",
    "Topic 1\n",
    "0.0301\tengineering\n",
    "0.0296\tdesign\n",
    "0.0204\tscience\n",
    "0.0204\tmarketing\n",
    "0.0195\tselection\n",
    "0.0191\tproduct\n",
    "0.0107\tused\n",
    "0.0107\talternative\n",
    "0.0107\teach\n",
    "0.0107\tpresent\n",
    "\n",
    "Topic 2\n",
    "0.0302\tInvestigator\n",
    "0.0302\tpreferences\n",
    "0.0302\tproduct\n",
    "0.0158\tselection\n",
    "0.0158\tuncertainties\n",
    "0.0158\tapproach\n",
    "0.0158\tPrincipal\n",
    "0.0158\tdesigns\n",
    "0.0158\tmethodology\n",
    "0.0158\tprototype\n",
    "\n",
    "Topic 3\n",
    "0.0359\tdesign\n",
    "0.0359\talternative\n",
    "0.0243\twill\n",
    "0.0215\tthis\n",
    "0.0127\tproduct\n",
    "0.0127\tpractices\n",
    "0.0127\tgeneration\n",
    "0.0127\tstages\n",
    "0.0127\trecursive\n",
    "0.0127\tinvestigation\n",
    "\n",
    "Topic 4\n",
    "0.0281\tdesign\n",
    "0.0281\tstage\n",
    "0.0281\tmarket\n",
    "0.0281\tcurrent\n",
    "0.0281\tmodule\n",
    "0.0281\tInvestigator\n",
    "0.0281\tPrincipal\n",
    "0.0281\tCo\n",
    "0.0147\tProduct\n",
    "0.0147\talternatives\n",
    "\n",
    "Topic 5\n",
    "0.0264\tparameters\n",
    "0.0264\tgenerated\n",
    "0.0264\teach\n",
    "0.0264\tdesign\n",
    "0.0138\tengineering\n",
    "0.0138\tframework\n",
    "0.0138\testimates\n",
    "0.0138\tuncontrollable\n",
    "0.0138\tset\n",
    "0.0138\tcustomer\n",
    "\n",
    "Topic 6\n",
    "0.0318\tdesign\n",
    "0.0318\tuncertainty\n",
    "0.0318\tformulation\n",
    "0.0318\tengineering\n",
    "0.0318\tfed\n",
    "0.0318\tinto\n",
    "0.0318\tAmendment\n",
    "0.0029\tExpected\n",
    "0.0029\tLatest\n",
    "0.0029\tIf\n",
    "\n",
    "Topic 7\n",
    "0.0205\tinto\n",
    "0.0205\tset\n",
    "0.0205\tUniversity\n",
    "0.0205\tobtain\n",
    "0.0205\tMaryland\n",
    "0.0205\tcourses\n",
    "0.0205\trobust\n",
    "0.0205\tknowledge\n",
    "0.0205\tsimulation\n",
    "0.0205\tintegrated\n",
    "\n",
    "Topic 8\n",
    "0.0254\tresearch\n",
    "0.0254\trobust\n",
    "0.0254\tAward\n",
    "0.0133\tadvance\n",
    "0.0133\tobjective\n",
    "0.0133\tPrincipal\n",
    "0.0133\tstate\n",
    "0.0133\tCo\n",
    "0.0133\tsuccessful\n",
    "0.0133\twill\n",
    "\n",
    "Topic 9\n",
    "0.0288\tCollege\n",
    "0.0288\t405\n",
    "0.0288\tPark\n",
    "0.0288\tMD\n",
    "0.0288\t6269\n",
    "0.0288\t207425141\n",
    "0.0288\t2002\n",
    "0.0288\tJune\n",
    "0.0288\tStart\n",
    "0.0288\tDate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar terms to: with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9999999999999999, 'world'),\n",
       " (0.9999999999999999, 'into'),\n",
       " (0.5029713360043134, 'mathematics'),\n",
       " (0.3551236073883025, 'task'),\n",
       " (0.11699697043571157, 'of'),\n",
       " (0.10912121868680959, 'the'),\n",
       " (0.0, 'with'),\n",
       " (0.0, 'which'),\n",
       " (0.0, 'variety'),\n",
       " (0.0, 'varieties')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Similar terms to:\", features[80])\n",
    "# Get most similar terms according to the cosine similarity of their vectors (columns in the term-document matrix)\n",
    "heapq.nlargest(10, zip(cosine_similarity(tfidf_matrix[:,81].todense().T, tfidf_matrix.todense().T)[0], features))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Similar terms to: with\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Out[24]:\n",
    "\n",
    "[(0.9999999999999999, 'world'),\n",
    " (0.9999999999999999, 'into'),\n",
    " (0.5029713360043134, 'mathematics'),\n",
    " (0.3551236073883025, 'task'),\n",
    " (0.11699697043571157, 'of'),\n",
    " (0.10912121868680959, 'the'),\n",
    " (0.0, 'with'),\n",
    " (0.0, 'which'),\n",
    " (0.0, 'variety'),\n",
    " (0.0, 'varieties')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 22:56:14,776 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2020-03-12 22:56:14,778 : INFO : collecting all words and their counts\n",
      "2020-03-12 22:56:14,779 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-12 22:56:14,781 : INFO : collected 246 word types from a corpus of 421 raw words and 65 sentences\n",
      "2020-03-12 22:56:14,782 : INFO : Loading a fresh vocabulary\n",
      "2020-03-12 22:56:14,786 : INFO : effective_min_count=1 retains 246 unique words (100% of original 246, drops 0)\n",
      "2020-03-12 22:56:14,787 : INFO : effective_min_count=1 leaves 421 word corpus (100% of original 421, drops 0)\n",
      "2020-03-12 22:56:14,793 : INFO : deleting the raw counts dictionary of 246 items\n",
      "2020-03-12 22:56:14,794 : INFO : sample=0.001 downsamples 65 most-common words\n",
      "2020-03-12 22:56:14,795 : INFO : downsampling leaves estimated 284 word corpus (67.6% of prior 421)\n",
      "2020-03-12 22:56:14,798 : INFO : estimated required memory for 246 words and 10 dimensions: 142680 bytes\n",
      "2020-03-12 22:56:14,799 : INFO : resetting layer weights\n",
      "2020-03-12 22:56:14,816 : INFO : training model with 4 workers on 246 vocabulary and 10 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-12 22:56:14,876 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 22:56:14,877 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 22:56:14,879 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 22:56:14,880 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 22:56:14,881 : INFO : EPOCH - 1 : training on 421 raw words (288 effective words) took 0.0s, 43123 effective words/s\n",
      "2020-03-12 22:56:14,890 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 22:56:14,891 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 22:56:14,893 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 22:56:14,894 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 22:56:14,895 : INFO : EPOCH - 2 : training on 421 raw words (288 effective words) took 0.0s, 40335 effective words/s\n",
      "2020-03-12 22:56:14,905 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 22:56:14,907 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 22:56:14,909 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 22:56:14,910 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 22:56:14,912 : INFO : EPOCH - 3 : training on 421 raw words (284 effective words) took 0.0s, 32966 effective words/s\n",
      "2020-03-12 22:56:14,918 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 22:56:14,921 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 22:56:14,923 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 22:56:14,925 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 22:56:14,927 : INFO : EPOCH - 4 : training on 421 raw words (282 effective words) took 0.0s, 31016 effective words/s\n",
      "2020-03-12 22:56:14,934 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-12 22:56:14,936 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-12 22:56:14,937 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-12 22:56:14,938 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-12 22:56:14,940 : INFO : EPOCH - 5 : training on 421 raw words (285 effective words) took 0.0s, 42907 effective words/s\n",
      "2020-03-12 22:56:14,941 : INFO : training on a 2105 raw words (1427 effective words) took 0.1s, 11524 effective words/s\n",
      "2020-03-12 22:56:14,942 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "### Train word vectors\n",
    "\n",
    "import gensim # Make sure you also have cython installed to accelerate computation!\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Train word2vec model\n",
    "vectors = gensim.models.Word2Vec(tokenized_text, size=10, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01109278, -0.04341133, -0.04535558, -0.04294013,  0.0078231 ,\n",
       "       -0.0369525 ,  0.03803169, -0.02085755,  0.00150449, -0.04307546],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a word vector\n",
    "vectors.wv[\"uncertainties\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([-0.01109278, -0.04341133, -0.04535558, -0.04294013,  0.0078231 ,\n",
    "       -0.0369525 ,  0.03803169, -0.02085755,  0.00150449, -0.04307546],\n",
    "      dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 22:56:25,016 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: uncertainties\n",
      "[('the', 0.7794070243835449), ('alternative', 0.7344417572021484), ('207425141', 0.7154356241226196), ('this', 0.6911375522613525), ('0200029', 0.6902447938919067), ('automation', 0.6342875957489014), ('robust', 0.6024630069732666), ('courses', 0.581272304058075), ('or', 0.5742402076721191), ('Title', 0.5704677700996399)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect words with vectors most similar to a given word\n",
    "print(\"Most similar to:\", 'uncertainties')\n",
    "print(vectors.wv.most_similar('uncertainties'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Most similar to: uncertainties\n",
    "[('the', 0.7794070243835449), ('alternative', 0.7344417572021484), ('207425141', 0.7154356241226196), ('this', 0.6911375522613525), ('0200029', 0.6902447938919067), ('automation', 0.6342875957489014), ('robust', 0.6024630069732666), ('courses', 0.581272304058075), ('or', 0.5742402076721191), ('Title', 0.5704677700996399)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: ['process', 'uncertainties']\n",
      "[('207425141', 0.7552006840705872), ('March', 0.7216493487358093), ('alternative', 0.7198970913887024), ('domains', 0.7008569240570068), ('this', 0.6458460092544556), ('Competitive', 0.6448894739151001), ('enhance', 0.6377053260803223), ('The', 0.6317423582077026), ('Based', 0.6279781460762024), ('automation', 0.5887629985809326)]\n"
     ]
    }
   ],
   "source": [
    "# ...or combination or words (average vector)\n",
    "print(\"Most similar to:\", ['process', 'uncertainties'])\n",
    "print(vectors.wv.most_similar(['process', 'uncertainties']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Most similar to: ['process', 'uncertainties']\n",
    "[('207425141', 0.7552006840705872), ('March', 0.7216493487358093), ('alternative', 0.7198970913887024), ('domains', 0.7008569240570068), ('this', 0.6458460092544556), ('Competitive', 0.6448894739151001), ('enhance', 0.6377053260803223), ('The', 0.6317423582077026), ('Based', 0.6279781460762024), ('automation', 0.5887629985809326)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36967823\n",
      "-0.20987485\n",
      "0.04629732\n",
      "-0.30490637\n"
     ]
    }
   ],
   "source": [
    "# Inspect cosine similarities between specific words\n",
    "print(vectors.wv.similarity('technology', 'design'))\n",
    "print(vectors.wv.similarity('technology', 'system'))\n",
    "print(vectors.wv.similarity('technology', 'Award'))\n",
    "print(vectors.wv.similarity('technology', 'time'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0.36967823\n",
    "-0.20987485\n",
    "0.04629732\n",
    "-0.30490637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Man is to King as Woman is to ...\\n# vec(woman)+vec(king)-vec(man)\\nprint(vectors.most_similar(['woman','king'], negative=['man']))\\nprint()\\n\\n# Fall is to Fell as Watch is to ...\\nprint(vectors.most_similar(['wear','fell'], negative=['fall']))\\nprint()\\n\\nprint(vectors.most_similar(['Moscow','France'], negative=['Paris']))\\nprint()\\n\\nprint(vectors.most_similar(['London','France'], negative=['Paris']))\\n\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test for analogies and syntactic regularities (when trained on a large and suitable corpus)\n",
    "\n",
    "\"\"\"\n",
    "# Man is to King as Woman is to ...\n",
    "# vec(woman)+vec(king)-vec(man)\n",
    "print(vectors.most_similar(['woman','king'], negative=['man']))\n",
    "print()\n",
    "\n",
    "# Fall is to Fell as Watch is to ...\n",
    "print(vectors.most_similar(['wear','fell'], negative=['fall']))\n",
    "print()\n",
    "\n",
    "print(vectors.most_similar(['Moscow','France'], negative=['Paris']))\n",
    "print()\n",
    "\n",
    "print(vectors.most_similar(['London','France'], negative=['Paris']))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: design\n",
      "[('learn', 0.9009382128715515), ('commonality', 0.8388251066207886), ('Org', 0.6409584283828735), ('generation', 0.6354787349700928), ('eng', 0.614288330078125), ('Technology', 0.6135535836219788), ('recommendations', 0.5996837615966797), ('decision', 0.5849766731262207), ('selection', 0.5335516333580017), ('Sponsor', 0.5313560962677002)]\n",
      "\n",
      "Most similar to: ['date', 'design']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 22:57:00,314 : INFO : NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'date' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-20d414e14dc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Are the below average vectors less ambiguous?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Most similar to:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'design'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'between'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'date' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# Ambiguous word vector examples\n",
    "\n",
    "print(\"Most similar to:\", 'design')\n",
    "print(vectors.wv.most_similar('design'))\n",
    "print()\n",
    "\n",
    "# Are the below average vectors less ambiguous?\n",
    "print(\"Most similar to:\", ['date','design'])\n",
    "print(vectors.wv.most_similar(['date','between']))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Most similar to: design\n",
    "[('learn', 0.9009382128715515), ('commonality', 0.8388251066207886), ('Org', 0.6409584283828735), ('generation', 0.6354787349700928), ('eng', 0.614288330078125), ('Technology', 0.6135535836219788), ('recommendations', 0.5996837615966797), ('decision', 0.5849766731262207), ('selection', 0.5335516333580017), ('Sponsor', 0.5313560962677002)]\n",
    "\n",
    "Most similar to: ['date', 'design']\n",
    "\n",
    "\"word 'date' not in vocabulary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install tensorflow_hub tensorflow\n",
    "\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "# Load ELMo model (takes a little while)\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "def elmo_vectors(sents):\n",
    "    embeddings = elmo(sents, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        return sess.run(embeddings)\n",
    "        #sess.run(tf.tables_initializer())\n",
    "        #return average of ELMo features as sentence vector\n",
    "        #return sess.run(tf.reduce_mean(embeddings,1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020-03-12 21:38:24,651 : INFO : Using C:\\Users\\shohidul\\AppData\\Local\\Temp\\tfhub_modules to cache modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-12 22:58:21,686 : INFO : Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: He wrote the play himself .\n",
      "Vector for 'play': [-0.27192304  0.11288124 -0.3028859  ... -0.27269745  0.04967653\n",
      " -0.02397385]\n",
      "\n",
      "Sentence: He did play with the kids .\n",
      "Vector for 'play': [ 0.14034374  0.75535333  0.5396338  ... -0.26539382  0.25861102\n",
      "  0.21444729]\n",
      "\n",
      "Sentence: His excuse didn't play well .\n",
      "Vector for 'play': [-0.08138274  0.13417737  0.41862282 ... -0.44263023  0.30514812\n",
      "  0.22920589]\n",
      "\n",
      "Sentence: I saw the play with him .\n",
      "Vector for 'play': [-0.42460334 -0.03112318 -0.01002066 ... -0.13054961 -0.06333602\n",
      "  0.08011345]\n",
      "\n",
      "Sentence: He can play it by ear .\n",
      "Vector for 'play': [ 0.06018784  0.6498274   0.5208825  ...  0.14704973  0.02895296\n",
      " -0.08715162]\n",
      "\n",
      "Word vector size: (1024,)\n"
     ]
    }
   ],
   "source": [
    "# Get contextualized vectors for target word in different sentences\n",
    "\n",
    "sents = \"\"\"He wrote the play himself .\n",
    "He did play with the kids .\n",
    "His excuse didn't play well .\n",
    "I saw the play with him .\n",
    "He can play it by ear .\"\"\".split('\\n')\n",
    "\n",
    "target = 'play'\n",
    "\n",
    "elmo_vecs = elmo_vectors(sents)\n",
    "word_vecs = []\n",
    "for i, sent in enumerate(sents):\n",
    "    word_vecs.append(elmo_vecs[i][sent.split().index(target)])\n",
    "    print(\"Sentence:\", sent)\n",
    "    print(\"Vector for '%s':\" % target, word_vecs[-1])\n",
    "    print()\n",
    "\n",
    "print(\"Word vector size:\", word_vecs[0].shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sentence: He wrote the play himself .\n",
    "Vector for 'play': [-0.27192304  0.11288124 -0.3028859  ... -0.27269745  0.04967653\n",
    " -0.02397385]\n",
    "\n",
    "Sentence: He did play with the kids .\n",
    "Vector for 'play': [ 0.14034374  0.75535333  0.5396338  ... -0.26539382  0.25861102\n",
    "  0.21444729]\n",
    "\n",
    "Sentence: His excuse didn't play well .\n",
    "Vector for 'play': [-0.08138274  0.13417737  0.41862282 ... -0.44263023  0.30514812\n",
    "  0.22920589]\n",
    "\n",
    "Sentence: I saw the play with him .\n",
    "Vector for 'play': [-0.42460334 -0.03112318 -0.01002066 ... -0.13054961 -0.06333602\n",
    "  0.08011345]\n",
    "\n",
    "Sentence: He can play it by ear .\n",
    "Vector for 'play': [ 0.06018784  0.6498274   0.5208825  ...  0.14704973  0.02895296\n",
    " -0.08715162]\n",
    "\n",
    "Word vector size: (1024,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities between 'play' vector in sentences:\n",
      "Sent 0-1: 0.6555581\n",
      "Sent 0-2: 0.60003823\n",
      "Sent 0-3: 0.74981964\n",
      "Sent 0-4: 0.5564689\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vec_size = word_vecs[0].shape[0]\n",
    "print(\"Similarities between '%s' vector in sentences:\" % target)\n",
    "for i in range(1, len(sents)):\n",
    "    print(\"Sent 0-%d:\" % i, cosine_similarity(word_vecs[0].reshape((1,vec_size)), \n",
    "                                              word_vecs[i].reshape((1,vec_size)))[0][0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Similarities between 'play' vector in sentences:\n",
    "Sent 0-1: 0.6555581\n",
    "Sent 0-2: 0.60003823\n",
    "Sent 0-3: 0.74981964\n",
    "Sent 0-4: 0.5564689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
